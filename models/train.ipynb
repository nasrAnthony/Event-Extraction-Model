{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e833499f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rendering_order</th>\n",
       "      <th>depth</th>\n",
       "      <th>parent_index</th>\n",
       "      <th>text_length</th>\n",
       "      <th>sibling_index</th>\n",
       "      <th>children_count</th>\n",
       "      <th>same_tag_sibling_count</th>\n",
       "      <th>same_text_sibling_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>letter_ratio</th>\n",
       "      <th>digit_ratio</th>\n",
       "      <th>whitespace_ratio</th>\n",
       "      <th>attribute_count</th>\n",
       "      <th>event_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2764.000000</td>\n",
       "      <td>2764.000000</td>\n",
       "      <td>2764.000000</td>\n",
       "      <td>2764.000000</td>\n",
       "      <td>2764.000000</td>\n",
       "      <td>2764.000000</td>\n",
       "      <td>2764.000000</td>\n",
       "      <td>2764.000000</td>\n",
       "      <td>2764.000000</td>\n",
       "      <td>2764.000000</td>\n",
       "      <td>2764.00000</td>\n",
       "      <td>2764.000000</td>\n",
       "      <td>2764.000000</td>\n",
       "      <td>789.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>665.224674</td>\n",
       "      <td>17.407742</td>\n",
       "      <td>659.378075</td>\n",
       "      <td>26.265557</td>\n",
       "      <td>0.778220</td>\n",
       "      <td>0.140376</td>\n",
       "      <td>1.033285</td>\n",
       "      <td>0.001447</td>\n",
       "      <td>4.020984</td>\n",
       "      <td>0.767786</td>\n",
       "      <td>0.12120</td>\n",
       "      <td>0.075350</td>\n",
       "      <td>0.921129</td>\n",
       "      <td>11.693283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>528.325947</td>\n",
       "      <td>6.531681</td>\n",
       "      <td>526.280103</td>\n",
       "      <td>43.190741</td>\n",
       "      <td>2.411446</td>\n",
       "      <td>0.520146</td>\n",
       "      <td>3.663405</td>\n",
       "      <td>0.038021</td>\n",
       "      <td>6.543288</td>\n",
       "      <td>0.302293</td>\n",
       "      <td>0.27128</td>\n",
       "      <td>0.061539</td>\n",
       "      <td>1.733493</td>\n",
       "      <td>9.542569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>314.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>307.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>514.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>507.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.885714</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>837.500000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>833.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2375.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>2366.000000</td>\n",
       "      <td>559.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>42.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       rendering_order        depth  parent_index  text_length  sibling_index  \\\n",
       "count      2764.000000  2764.000000   2764.000000  2764.000000    2764.000000   \n",
       "mean        665.224674    17.407742    659.378075    26.265557       0.778220   \n",
       "std         528.325947     6.531681    526.280103    43.190741       2.411446   \n",
       "min          19.000000     2.000000     18.000000     1.000000       0.000000   \n",
       "25%         314.000000    12.000000    307.000000     8.000000       0.000000   \n",
       "50%         514.000000    17.000000    507.000000    16.000000       0.000000   \n",
       "75%         837.500000    23.000000    833.000000    27.000000       0.000000   \n",
       "max        2375.000000    30.000000   2366.000000   559.000000      25.000000   \n",
       "\n",
       "       children_count  same_tag_sibling_count  same_text_sibling_count  \\\n",
       "count     2764.000000             2764.000000              2764.000000   \n",
       "mean         0.140376                1.033285                 0.001447   \n",
       "std          0.520146                3.663405                 0.038021   \n",
       "min          0.000000                0.000000                 0.000000   \n",
       "25%          0.000000                0.000000                 0.000000   \n",
       "50%          0.000000                0.000000                 0.000000   \n",
       "75%          0.000000                0.000000                 0.000000   \n",
       "max         11.000000               25.000000                 1.000000   \n",
       "\n",
       "        word_count  letter_ratio  digit_ratio  whitespace_ratio  \\\n",
       "count  2764.000000   2764.000000   2764.00000       2764.000000   \n",
       "mean      4.020984      0.767786      0.12120          0.075350   \n",
       "std       6.543288      0.302293      0.27128          0.061539   \n",
       "min       1.000000      0.000000      0.00000          0.000000   \n",
       "25%       1.000000      0.785714      0.00000          0.000000   \n",
       "50%       2.000000      0.885714      0.00000          0.083333   \n",
       "75%       4.000000      0.941176      0.00000          0.117647   \n",
       "max      81.000000      1.000000      1.00000          0.333333   \n",
       "\n",
       "       attribute_count    event_id  \n",
       "count      2764.000000  789.000000  \n",
       "mean          0.921129   11.693283  \n",
       "std           1.733493    9.542569  \n",
       "min           0.000000    1.000000  \n",
       "25%           0.000000    4.000000  \n",
       "50%           1.000000    8.000000  \n",
       "75%           1.000000   19.000000  \n",
       "max          15.000000   42.000000  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "data_path = os.path.join(os.getcwd(), '..', 'data', 'cleaned', 'full_data.csv')\n",
    "df = pd.read_csv(data_path)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7a4d439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages: 15\n",
      "Total nodes: 2764\n",
      "start_event positives: 177\n",
      "Label counts:\n",
      " label\n",
      "Other           1976\n",
      "Date             267\n",
      "Location         121\n",
      "StartEndTime     103\n",
      "Name              84\n",
      "NameLink          60\n",
      "Description       57\n",
      "DateTime          47\n",
      "EndTime           20\n",
      "StartTime         19\n",
      "NameLocation       6\n",
      "Time               2\n",
      "TimeLocation       2\n",
      "Name: count, dtype: int64\n",
      "Start_event positive rate: 0.06403762662807526\n",
      "Expected positives: 177 Actual positives: 177\n"
     ]
    }
   ],
   "source": [
    "# Sort within each page\n",
    "df = df.sort_values([\"source\", \"rendering_order\"]).reset_index(drop=True)\n",
    "\n",
    "#boundary label\n",
    "is_event = df[\"event_id\"].notna()\n",
    "\n",
    "df[\"start_event\"] = 0\n",
    "df.loc[is_event, \"start_event\"] = (\n",
    "    df[is_event]\n",
    "    .groupby([\"source\", \"event_id\"])\n",
    "    .cumcount()\n",
    "    .eq(0)\n",
    "    .astype(int)\n",
    ").values\n",
    "\n",
    "print(\"Pages:\", df[\"source\"].nunique())\n",
    "print(\"Total nodes:\", len(df))\n",
    "print(\"start_event positives:\", int(df[\"start_event\"].sum()))\n",
    "print(\"Label counts:\\n\", df[\"label\"].value_counts())\n",
    "\n",
    "# positive rate overall\n",
    "print(\"Start_event positive rate:\", df[\"start_event\"].mean())\n",
    "\n",
    "# sanity: should equal number of unique (source,event_id) pairs among events\n",
    "expected = df.loc[is_event, [\"source\",\"event_id\"]].drop_duplicates().shape[0]\n",
    "actual = int(df[\"start_event\"].sum())\n",
    "print(\"Expected positives:\", expected, \"Actual positives:\", actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb1c3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train pages: 11\n",
      "Val pages: 2\n",
      "Test pages: 2\n",
      "Train events: 117\n",
      "Val events: 8\n",
      "Test events: 52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1127/2849548259.py:6: UserWarning: you are shuffling a 'StringArray' object which is not a subclass of 'Sequence'; `shuffle` is not guaranteed to behave correctly. E.g., non-numpy array/tensor objects with view semantics may contain duplicates after shuffling.\n",
      "  rng.shuffle(sources)\n"
     ]
    }
   ],
   "source": [
    "# Get unique pages\n",
    "sources = df[\"source\"].unique()\n",
    "\n",
    "# Shuffle reproducibly\n",
    "rng = np.random.default_rng(42)\n",
    "rng.shuffle(sources)\n",
    "\n",
    "# 11 train / 2 val / 2 test (for 15 pages)\n",
    "test_sources = set(sources[:2])\n",
    "val_sources  = set(sources[2:4])\n",
    "train_sources= set(sources[4:])\n",
    "\n",
    "# Create splits\n",
    "train_df = df[df[\"source\"].isin(train_sources)].copy()\n",
    "val_df   = df[df[\"source\"].isin(val_sources)].copy()\n",
    "test_df  = df[df[\"source\"].isin(test_sources)].copy()\n",
    "\n",
    "print(\"Train pages:\", train_df[\"source\"].nunique())\n",
    "print(\"Val pages:\", val_df[\"source\"].nunique())\n",
    "print(\"Test pages:\", test_df[\"source\"].nunique())\n",
    "\n",
    "print(\"Train events:\",\n",
    "      train_df.dropna(subset=[\"event_id\"]).groupby([\"source\",\"event_id\"]).ngroups)\n",
    "print(\"Val events:\",\n",
    "      val_df.dropna(subset=[\"event_id\"]).groupby([\"source\",\"event_id\"]).ngroups)\n",
    "print(\"Test events:\",\n",
    "      test_df.dropna(subset=[\"event_id\"]).groupby([\"source\",\"event_id\"]).ngroups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf013315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "12.8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#check if cuda is available\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Labels: keep as-is\n",
    "LABELS = sorted(df[\"label\"].unique().tolist())\n",
    "label2id = {l:i for i,l in enumerate(LABELS)}\n",
    "id2label = {i:l for l,i in label2id.items()}\n",
    "\n",
    "# Vocab for categorical structural features\n",
    "TAG_VOCAB = {t:i for i,t in enumerate(sorted(df[\"tag\"].astype(str).unique().tolist()))}\n",
    "PARENT_TAG_VOCAB = {t:i for i,t in enumerate(sorted(df[\"parent_tag\"].astype(str).unique().tolist()))}\n",
    "\n",
    "# Structural columns (already in your CSV)\n",
    "STRUCT_COLS_NUM = [\n",
    "    \"depth\",\"sibling_index\",\"children_count\",\"same_tag_sibling_count\",\n",
    "    \"same_text_sibling_count\",\"text_length\",\"word_count\",\n",
    "    \"letter_ratio\",\"digit_ratio\",\"whitespace_ratio\",\"attribute_count\"\n",
    "]\n",
    "STRUCT_COLS_BOOL = [\n",
    "    \"has_link\",\"link_is_absolute\",\"parent_has_link\",\"is_leaf\",\n",
    "    \"contains_date\",\"contains_time\",\"starts_with_digit\",\"ends_with_digit\",\n",
    "    \"has_class\",\"has_id\",\n",
    "    \"attr_has_word_name\",\"attr_has_word_date\",\"attr_has_word_time\",\"attr_has_word_location\",\"attr_has_word_link\",\n",
    "    \"text_has_word_name\",\"text_has_word_date\",\"text_word_time\",\"text_word_description\",\"text_word_location\"\n",
    "]\n",
    "\n",
    "class PageDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.pages = []\n",
    "        for src, g in df.groupby(\"source\"):\n",
    "            g = g.sort_values(\"rendering_order\").reset_index(drop=True)\n",
    "            self.pages.append(g)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pages)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        g = self.pages[idx]\n",
    "        texts = g[\"text_context\"].astype(str).tolist()\n",
    "\n",
    "        field_y = torch.tensor([label2id[x] for x in g[\"label\"].tolist()], dtype=torch.long)\n",
    "        boundary_y = torch.tensor(g[\"start_event\"].astype(int).tolist(), dtype=torch.float32)\n",
    "\n",
    "        tag_id = torch.tensor([TAG_VOCAB[str(x)] for x in g[\"tag\"]], dtype=torch.long)\n",
    "        parent_tag_id = torch.tensor([PARENT_TAG_VOCAB[str(x)] for x in g[\"parent_tag\"]], dtype=torch.long)\n",
    "\n",
    "        num_feats = torch.tensor(g[STRUCT_COLS_NUM].fillna(0).values, dtype=torch.float32)\n",
    "        bool_feats = torch.tensor(g[STRUCT_COLS_BOOL].astype(int).values, dtype=torch.float32)\n",
    "\n",
    "        return {\n",
    "            \"texts\": texts,\n",
    "            \"field_y\": field_y,\n",
    "            \"boundary_y\": boundary_y,\n",
    "            \"tag_id\": tag_id,\n",
    "            \"parent_tag_id\": parent_tag_id,\n",
    "            \"num_feats\": num_feats,\n",
    "            \"bool_feats\": bool_feats,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "488ea22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tony/DTI5902/RP/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def collate_fn(batch, max_tokens=64):\n",
    "    B = len(batch)\n",
    "    max_nodes = max(len(x[\"texts\"]) for x in batch)\n",
    "\n",
    "    # node mask\n",
    "    node_mask = torch.zeros((B, max_nodes), dtype=torch.bool)\n",
    "    all_texts = []\n",
    "    node_offsets = []\n",
    "    for i, item in enumerate(batch):\n",
    "        n = len(item[\"texts\"])\n",
    "        node_mask[i, :n] = True\n",
    "        node_offsets.append((len(all_texts), len(all_texts) + n))\n",
    "        all_texts.extend(item[\"texts\"])\n",
    "\n",
    "    enc = tokenizer(\n",
    "        all_texts, padding=True, truncation=True, max_length=max_tokens, return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    def pad_1d(tensors, pad_value):\n",
    "        out = torch.full((B, max_nodes), pad_value, dtype=tensors[0].dtype)\n",
    "        for i, t in enumerate(tensors):\n",
    "            out[i, :len(t)] = t\n",
    "        return out\n",
    "\n",
    "    def pad_2d(tensors, feat_dim, pad_value=0.0):\n",
    "        out = torch.full((B, max_nodes, feat_dim), pad_value, dtype=tensors[0].dtype)\n",
    "        for i, t in enumerate(tensors):\n",
    "            out[i, :t.shape[0], :] = t\n",
    "        return out\n",
    "\n",
    "    field_y = pad_1d([x[\"field_y\"] for x in batch], pad_value=-100)   # ignore padding\n",
    "    boundary_y = pad_1d([x[\"boundary_y\"] for x in batch], pad_value=0)\n",
    "\n",
    "    tag_id = pad_1d([x[\"tag_id\"] for x in batch], pad_value=0)\n",
    "    parent_tag_id = pad_1d([x[\"parent_tag_id\"] for x in batch], pad_value=0)\n",
    "\n",
    "    num_feats = pad_2d([x[\"num_feats\"] for x in batch], len(STRUCT_COLS_NUM), 0.0)\n",
    "    bool_feats = pad_2d([x[\"bool_feats\"] for x in batch], len(STRUCT_COLS_BOOL), 0.0)\n",
    "\n",
    "    return {\n",
    "        \"enc\": enc,\n",
    "        \"node_offsets\": node_offsets,\n",
    "        \"node_mask\": node_mask,\n",
    "        \"field_y\": field_y,\n",
    "        \"boundary_y\": boundary_y,\n",
    "        \"tag_id\": tag_id,\n",
    "        \"parent_tag_id\": parent_tag_id,\n",
    "        \"num_feats\": num_feats,\n",
    "        \"bool_feats\": bool_feats,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "faf8eb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "\n",
    "class DOMAwareEventExtractor(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        text_model_name: str,\n",
    "        num_field_labels: int,\n",
    "        tag_vocab_size: int,\n",
    "        parent_tag_vocab_size: int,\n",
    "        d_model: int = 256,\n",
    "        nhead: int = 8,\n",
    "        num_layers: int = 4,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.text_encoder = AutoModel.from_pretrained(text_model_name)\n",
    "        text_dim = self.text_encoder.config.hidden_size\n",
    "\n",
    "        self.text_proj = nn.Linear(text_dim, d_model)\n",
    "\n",
    "        self.tag_emb = nn.Embedding(tag_vocab_size, d_model)\n",
    "        self.parent_tag_emb = nn.Embedding(parent_tag_vocab_size, d_model)\n",
    "\n",
    "        self.num_proj = nn.Linear(len(STRUCT_COLS_NUM), d_model)\n",
    "        self.bool_proj = nn.Linear(len(STRUCT_COLS_BOOL), d_model)\n",
    "\n",
    "        self.layernorm = nn.LayerNorm(d_model)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.node_encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
    "\n",
    "        self.field_head = nn.Linear(d_model, num_field_labels)\n",
    "        self.boundary_head = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, enc, node_offsets, node_mask, tag_id, parent_tag_id, num_feats, bool_feats):\n",
    "        out = self.text_encoder(**enc)\n",
    "        cls = out.last_hidden_state[:, 0, :]           # [total_nodes, text_dim]\n",
    "        node_text = self.text_proj(cls)                # [total_nodes, d_model]\n",
    "\n",
    "        B, max_nodes = node_mask.shape\n",
    "        packed = node_text.new_zeros((B, max_nodes, node_text.shape[-1]))\n",
    "        for i, (s, e) in enumerate(node_offsets):\n",
    "            packed[i, : (e - s), :] = node_text[s:e]\n",
    "\n",
    "        x = packed \\\n",
    "            + self.tag_emb(tag_id) \\\n",
    "            + self.parent_tag_emb(parent_tag_id) \\\n",
    "            + self.num_proj(num_feats) \\\n",
    "            + self.bool_proj(bool_feats)\n",
    "\n",
    "        x = self.layernorm(x)\n",
    "\n",
    "        key_padding_mask = ~node_mask\n",
    "        x = self.node_encoder(x, src_key_padding_mask=key_padding_mask)\n",
    "\n",
    "        field_logits = self.field_head(x)                    # [B, N, C]\n",
    "        boundary_logits = self.boundary_head(x).squeeze(-1)  # [B, N]\n",
    "        return field_logits, boundary_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfc1ef07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalLossWithLogits(nn.Module):\n",
    "    \"\"\"\n",
    "    Binary focal loss operating on logits.\n",
    "    alpha: weight for positive class (0..1). Often 0.25.\n",
    "    gamma: focusing parameter. Often 2.0.\n",
    "    reduction: 'mean' or 'sum'\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, reduction=\"mean\"):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        targets = targets.float()\n",
    "        bce = F.binary_cross_entropy_with_logits(logits, targets, reduction=\"none\")\n",
    "        p = torch.sigmoid(logits)\n",
    "        pt = torch.where(targets == 1, p, 1 - p)          # prob of the true class\n",
    "        alpha_t = torch.where(targets == 1, self.alpha, 1 - self.alpha)\n",
    "        loss = alpha_t * (1 - pt).pow(self.gamma) * bce\n",
    "\n",
    "        if self.reduction == \"mean\":\n",
    "            return loss.mean()\n",
    "        if self.reduction == \"sum\":\n",
    "            return loss.sum()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1045cb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Field class weights (inverse frequency)\n",
    "counts = Counter(df[\"label\"].tolist())\n",
    "w = torch.tensor([1.0 / (counts[l] + 1e-6) for l in LABELS], dtype=torch.float32)\n",
    "w = (w / w.sum()) * len(LABELS)   # normalize scale\n",
    "\n",
    "# Boundary pos_weight\n",
    "pos = float(df[\"start_event\"].sum())\n",
    "neg = float(len(df) - pos)\n",
    "pos_weight = torch.tensor([neg / (pos + 1e-6)], dtype=torch.float32).to(device)\n",
    "boundary_loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "label_counts = Counter(train_df[\"label\"])\n",
    "total = sum(label_counts.values())\n",
    "\n",
    "weights = []\n",
    "for label in LABELS:\n",
    "    count = label_counts.get(label, 1)\n",
    "    weights.append(total / count)\n",
    "\n",
    "weights = torch.tensor(weights, dtype=torch.float32).to(device)\n",
    "\n",
    "field_loss_fn = torch.nn.CrossEntropyLoss(\n",
    "    weight=weights,\n",
    "    ignore_index=-100\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4f80074",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 100/100 [00:00<00:00, 418.94it/s, Materializing param=transformer.layer.5.sa_layer_norm.weight]   \n",
      "\u001b[1mDistilBertModel LOAD REPORT\u001b[0m from: distilbert-base-uncased\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "vocab_layer_norm.weight | UNEXPECTED |  | \n",
      "vocab_projector.bias    | UNEXPECTED |  | \n",
      "vocab_transform.weight  | UNEXPECTED |  | \n",
      "vocab_layer_norm.bias   | UNEXPECTED |  | \n",
      "vocab_transform.bias    | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "/home/tony/DTI5902/RP/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:531: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
      "  output = torch._nested_tensor_from_mask(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Train loss: 3.1650\n",
      "Val loss:   2.8848\n",
      "Best boundary threshold: 0.05 Best val F1: 0.04651162786127636\n",
      "Epoch 2\n",
      "Train loss: 2.6886\n",
      "Val loss:   2.7773\n",
      "Best boundary threshold: 0.05 Best val F1: 0.04651162786127636\n",
      "Epoch 3\n",
      "Train loss: 2.6608\n",
      "Val loss:   2.6095\n",
      "Best boundary threshold: 0.05 Best val F1: 0.04651162786127636\n",
      "Epoch 4\n",
      "Train loss: 2.6013\n",
      "Val loss:   2.5364\n",
      "Best boundary threshold: 0.3 Best val F1: 0.04719764007163181\n",
      "Epoch 5\n",
      "Train loss: 2.4817\n",
      "Val loss:   2.4671\n",
      "Best boundary threshold: 0.35 Best val F1: 0.046920821068274256\n",
      "Epoch 6\n",
      "Train loss: 2.4041\n",
      "Val loss:   2.4285\n",
      "Best boundary threshold: 0.39999999999999997 Best val F1: 0.04733727806001191\n",
      "Epoch 7\n",
      "Train loss: 2.3314\n",
      "Val loss:   2.3617\n",
      "Best boundary threshold: 0.39999999999999997 Best val F1: 0.048632218797239495\n",
      "Epoch 8\n",
      "Train loss: 2.2350\n",
      "Val loss:   2.2605\n",
      "Best boundary threshold: 0.39999999999999997 Best val F1: 0.057347670194781666\n",
      "Epoch 9\n",
      "Train loss: 2.1469\n",
      "Val loss:   2.2026\n",
      "Best boundary threshold: 0.35 Best val F1: 0.05228758164808407\n",
      "Epoch 10\n",
      "Train loss: 2.0807\n",
      "Val loss:   2.1839\n",
      "Best boundary threshold: 0.35 Best val F1: 0.05280528047629317\n",
      "Epoch 11\n",
      "Train loss: 1.9622\n",
      "Val loss:   1.9808\n",
      "Best boundary threshold: 0.35 Best val F1: 0.05280528047629317\n",
      "Epoch 12\n",
      "Train loss: 1.8186\n",
      "Val loss:   1.8153\n",
      "Best boundary threshold: 0.35 Best val F1: 0.053691275115175\n",
      "Epoch 13\n",
      "Train loss: 1.6365\n",
      "Val loss:   1.7839\n",
      "Best boundary threshold: 0.35 Best val F1: 0.058394160526826146\n",
      "Epoch 14\n",
      "Train loss: 1.4697\n",
      "Val loss:   1.7478\n",
      "Best boundary threshold: 0.35 Best val F1: 0.06557377042784196\n",
      "Epoch 15\n",
      "Train loss: 1.4508\n",
      "Val loss:   1.5521\n",
      "Best boundary threshold: 0.35 Best val F1: 0.07253886002341003\n",
      "Epoch 16\n",
      "Train loss: 1.2655\n",
      "Val loss:   1.4369\n",
      "Best boundary threshold: 0.35 Best val F1: 0.07894736832029087\n",
      "Epoch 17\n",
      "Train loss: 1.2159\n",
      "Val loss:   1.3963\n",
      "Best boundary threshold: 0.35 Best val F1: 0.07329842923834325\n",
      "Epoch 18\n",
      "Train loss: 1.0013\n",
      "Val loss:   1.2495\n",
      "Best boundary threshold: 0.35 Best val F1: 0.12173913030321362\n",
      "Epoch 19\n",
      "Train loss: 0.9477\n",
      "Val loss:   1.1475\n",
      "Best boundary threshold: 0.3 Best val F1: 0.11965811952867265\n",
      "Epoch 20\n",
      "Train loss: 0.7990\n",
      "Val loss:   1.0923\n",
      "Best boundary threshold: 0.39999999999999997 Best val F1: 0.22222222192000005\n",
      "Epoch 21\n",
      "Train loss: 0.7167\n",
      "Val loss:   0.9688\n",
      "Best boundary threshold: 0.35 Best val F1: 0.2499999997118056\n",
      "Epoch 22\n",
      "Train loss: 0.7266\n",
      "Val loss:   0.9550\n",
      "Best boundary threshold: 0.39999999999999997 Best val F1: 0.29999999966500007\n",
      "Epoch 23\n",
      "Train loss: 0.6276\n",
      "Val loss:   0.8729\n",
      "Best boundary threshold: 0.35 Best val F1: 0.31111111080493825\n",
      "Epoch 24\n",
      "Train loss: 0.5394\n",
      "Val loss:   0.7830\n",
      "Best boundary threshold: 0.39999999999999997 Best val F1: 0.39999999958222215\n",
      "Epoch 25\n",
      "Train loss: 0.5467\n",
      "Val loss:   0.8189\n",
      "Best boundary threshold: 0.39999999999999997 Best val F1: 0.43243243207012416\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = PageDataset(train_df)\n",
    "val_dataset   = PageDataset(val_df)\n",
    "test_dataset  = PageDataset(test_df)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True,  collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "model = DOMAwareEventExtractor(\n",
    "    text_model_name=MODEL_NAME,\n",
    "    num_field_labels=len(LABELS),\n",
    "    tag_vocab_size=len(TAG_VOCAB),\n",
    "    parent_tag_vocab_size=len(PARENT_TAG_VOCAB),\n",
    "    d_model=256,\n",
    "    nhead=8,\n",
    "    num_layers=4\n",
    ").to(device)\n",
    "\n",
    "# IMPORTANT: ensure any weight tensors are on the right device\n",
    "# (If your field_loss_fn is CrossEntropyLoss(weight=...), make sure its weight tensor is on device)\n",
    "try:\n",
    "    if hasattr(field_loss_fn, \"weight\") and field_loss_fn.weight is not None:\n",
    "        field_loss_fn.weight = field_loss_fn.weight.to(device)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# IMPORTANT: recreate BCEWithLogitsLoss AFTER moving pos_weight to device\n",
    "pos_weight = pos_weight.to(device)\n",
    "boundary_loss_fn = FocalLossWithLogits(alpha=0.85, gamma=2.0).to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def find_best_threshold(loader):\n",
    "    model.eval()\n",
    "    all_probs, all_true = [], []\n",
    "\n",
    "    for batch in loader:\n",
    "        enc = {k:v.to(device) for k,v in batch[\"enc\"].items()}\n",
    "        mask = batch[\"node_mask\"].to(device).bool()\n",
    "\n",
    "        field_logits, boundary_logits = model(\n",
    "            enc=enc,\n",
    "            node_offsets=batch[\"node_offsets\"],\n",
    "            node_mask=mask,\n",
    "            tag_id=batch[\"tag_id\"].to(device),\n",
    "            parent_tag_id=batch[\"parent_tag_id\"].to(device),\n",
    "            num_feats=batch[\"num_feats\"].to(device),\n",
    "            bool_feats=batch[\"bool_feats\"].to(device)\n",
    "        )\n",
    "\n",
    "        probs = torch.sigmoid(boundary_logits[mask]).detach().cpu().numpy()\n",
    "        true  = batch[\"boundary_y\"][mask.cpu()].numpy().astype(int)\n",
    "\n",
    "        all_probs.append(probs)\n",
    "        all_true.append(true)\n",
    "\n",
    "    probs = np.concatenate(all_probs)\n",
    "    true  = np.concatenate(all_true)\n",
    "\n",
    "    best_th, best_f1 = 0.5, -1.0\n",
    "    for th in np.linspace(0.05, 0.95, 19):\n",
    "        pred = (probs >= th).astype(int)\n",
    "        tp = ((pred == 1) & (true == 1)).sum()\n",
    "        fp = ((pred == 1) & (true == 0)).sum()\n",
    "        fn = ((pred == 0) & (true == 1)).sum()\n",
    "        precision = tp / (tp + fp + 1e-9)\n",
    "        recall    = tp / (tp + fn + 1e-9)\n",
    "        f1        = 2 * precision * recall / (precision + recall + 1e-9)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_th = th\n",
    "\n",
    "    return best_th, best_f1\n",
    "\n",
    "def run_epoch(loader, training: bool = True):\n",
    "    model.train() if training else model.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in loader:\n",
    "        enc = {k: v.to(device) for k, v in batch[\"enc\"].items()}\n",
    "        node_mask = batch[\"node_mask\"].to(device).bool()  # FIX: force boolean mask\n",
    "\n",
    "        tag_id = batch[\"tag_id\"].to(device)\n",
    "        parent_tag_id = batch[\"parent_tag_id\"].to(device)\n",
    "        num_feats = batch[\"num_feats\"].to(device)\n",
    "        bool_feats = batch[\"bool_feats\"].to(device)\n",
    "\n",
    "        field_y = batch[\"field_y\"].to(device)\n",
    "        boundary_y = batch[\"boundary_y\"].to(device)\n",
    "\n",
    "        with torch.set_grad_enabled(training):\n",
    "            field_logits, boundary_logits = model(\n",
    "                enc=enc,\n",
    "                node_offsets=batch[\"node_offsets\"],\n",
    "                node_mask=node_mask,\n",
    "                tag_id=tag_id,\n",
    "                parent_tag_id=parent_tag_id,\n",
    "                num_feats=num_feats,\n",
    "                bool_feats=bool_feats\n",
    "            )\n",
    "\n",
    "            # FIX: mask out padding for BOTH losses\n",
    "            field_loss = field_loss_fn(\n",
    "                field_logits[node_mask],     # [num_real_nodes, C]\n",
    "                field_y[node_mask]           # [num_real_nodes]\n",
    "            )\n",
    "\n",
    "            boundary_loss = boundary_loss_fn(\n",
    "                boundary_logits[node_mask],  # [num_real_nodes]\n",
    "                boundary_y[node_mask].float()\n",
    "            )\n",
    "\n",
    "            loss = field_loss + 3.0 * boundary_loss\n",
    "\n",
    "            if training:\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # optional but helpful\n",
    "                optimizer.step()\n",
    "\n",
    "        total_loss += loss.detach().item()\n",
    "\n",
    "    return total_loss / max(1, len(loader))\n",
    "\n",
    "\n",
    "for epoch in range(25):\n",
    "    train_loss = run_epoch(train_loader, training=True)\n",
    "    val_loss   = run_epoch(val_loader, training=False)\n",
    "    best_th, best_val_f1 = find_best_threshold(val_loader)\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    print(f\"Train loss: {train_loss:.4f}\")\n",
    "    print(f\"Val loss:   {val_loss:.4f}\")\n",
    "    print(\"Best boundary threshold:\", best_th, \"Best val F1:\", best_val_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd4eec9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using best_th from val: 0.39999999999999997 best_val_f1: 0.43243243207012416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Field Label Metrics (node-level) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Date     0.4121    0.6071    0.4910       112\n",
      "    DateTime     0.0000    0.0000    0.0000         0\n",
      " Description     0.2083    0.1667    0.1852        30\n",
      "     EndTime     0.0000    0.0000    0.0000         0\n",
      "    Location     0.8780    0.8182    0.8471        44\n",
      "        Name     0.0000    0.0000    0.0000         0\n",
      "    NameLink     0.1020    0.1667    0.1266        30\n",
      "NameLocation     0.0000    0.0000    0.0000         0\n",
      "       Other     0.9350    0.7554    0.8357       552\n",
      "StartEndTime     0.5116    0.4231    0.4632        52\n",
      "   StartTime     0.0000    0.0000    0.0000         0\n",
      "        Time     0.0000    0.0000    0.0000         0\n",
      "TimeLocation     0.0000    0.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.6744       820\n",
      "   macro avg     0.2344    0.2259    0.2268       820\n",
      "weighted avg     0.7766    0.6744    0.7158       820\n",
      "\n",
      "=== Boundary Metrics (node-level) ===\n",
      "threshold=0.39999999999999997\n",
      "Precision: 0.1242  Recall: 0.3846  F1: 0.1878\n",
      "\n",
      "Loss: 2.4894\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_on_loader(loader, threshold=0.5, boundary_weight=3.0):\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    all_field_true, all_field_pred = [], []\n",
    "    all_bound_true, all_bound_pred = [], []\n",
    "\n",
    "    for batch in loader:\n",
    "        enc = {k:v.to(device) for k,v in batch[\"enc\"].items()}\n",
    "        node_mask = batch[\"node_mask\"].to(device).bool()\n",
    "\n",
    "        tag_id = batch[\"tag_id\"].to(device)\n",
    "        parent_tag_id = batch[\"parent_tag_id\"].to(device)\n",
    "        num_feats = batch[\"num_feats\"].to(device)\n",
    "        bool_feats = batch[\"bool_feats\"].to(device)\n",
    "\n",
    "        field_y = batch[\"field_y\"].to(device)\n",
    "        boundary_y = batch[\"boundary_y\"].to(device)\n",
    "\n",
    "        field_logits, boundary_logits = model(\n",
    "            enc=enc,\n",
    "            node_offsets=batch[\"node_offsets\"],\n",
    "            node_mask=node_mask,\n",
    "            tag_id=tag_id,\n",
    "            parent_tag_id=parent_tag_id,\n",
    "            num_feats=num_feats,\n",
    "            bool_feats=bool_feats\n",
    "        )\n",
    "\n",
    "        # ---- losses (match training) ----\n",
    "        field_loss = field_loss_fn(field_logits[node_mask], field_y[node_mask])\n",
    "        boundary_loss = boundary_loss_fn(boundary_logits[node_mask], boundary_y[node_mask].float())\n",
    "        loss = field_loss + boundary_weight * boundary_loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # ---- predictions ----\n",
    "        field_pred = torch.argmax(field_logits, dim=-1)\n",
    "\n",
    "        bound_prob = torch.sigmoid(boundary_logits)\n",
    "        bound_pred = (bound_prob >= threshold).long()\n",
    "\n",
    "        # gather valid nodes\n",
    "        valid = node_mask & (field_y != -100)\n",
    "\n",
    "        all_field_true.extend(field_y[valid].detach().cpu().tolist())\n",
    "        all_field_pred.extend(field_pred[valid].detach().cpu().tolist())\n",
    "\n",
    "        all_bound_true.extend(boundary_y[node_mask].detach().cpu().long().tolist())\n",
    "        all_bound_pred.extend(bound_pred[node_mask].detach().cpu().tolist())\n",
    "\n",
    "    avg_loss = total_loss / max(1, len(loader))\n",
    "\n",
    "    print(\"=== Field Label Metrics (node-level) ===\")\n",
    "    print(classification_report(\n",
    "        all_field_true,\n",
    "        all_field_pred,\n",
    "        labels=list(range(len(LABELS))),\n",
    "        target_names=[id2label[i] for i in range(len(LABELS))],\n",
    "        digits=4,\n",
    "        zero_division=0\n",
    "    ))\n",
    "\n",
    "    p, r, f1, _ = precision_recall_fscore_support(\n",
    "        all_bound_true, all_bound_pred, average=\"binary\", zero_division=0\n",
    "    )\n",
    "    print(\"=== Boundary Metrics (node-level) ===\")\n",
    "    print(f\"threshold={threshold}\")\n",
    "    print(f\"Precision: {p:.4f}  Recall: {r:.4f}  F1: {f1:.4f}\")\n",
    "    print(f\"\\nLoss: {avg_loss:.4f}\")\n",
    "\n",
    "    return avg_loss, (p, r, f1)\n",
    "\n",
    "# 1) pick best threshold on val\n",
    "best_th, best_val_f1 = find_best_threshold(val_loader)\n",
    "print(\"Using best_th from val:\", best_th, \"best_val_f1:\", best_val_f1)\n",
    "\n",
    "# 2) evaluate test using that threshold (NOT 0.5)\n",
    "_ = eval_on_loader(test_loader, threshold=best_th, boundary_weight=3.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
