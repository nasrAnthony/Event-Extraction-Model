{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e833499f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import math\n",
    "import hashlib\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e8fab13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 0) Load data\n",
    "# =============================================================================\n",
    "data_path = os.path.join(os.getcwd(), \"..\", \"data\", \"cleaned\", \"full_data.csv\")\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# =============================================================================\n",
    "# 1) Label merge (APPLY IT)\n",
    "# =============================================================================\n",
    "LABEL_MERGE_MAP = {\n",
    "    # keep\n",
    "    \"Other\": \"Other\",\n",
    "\n",
    "    # Name variants\n",
    "    \"Name\": \"Name\",\n",
    "    \"NameLink\": \"Name\",\n",
    "    \"NameLocation\": \"Name\",\n",
    "\n",
    "    # Date variants\n",
    "    \"Date\": \"Date\",\n",
    "    \"DateTime\": \"Date\",\n",
    "\n",
    "    # Time variants\n",
    "    \"Time\": \"Time\",\n",
    "    \"StartTime\": \"Time\",\n",
    "    \"EndTime\": \"Time\",\n",
    "    \"StartEndTime\": \"Time\",\n",
    "    \"TimeLocation\": \"Time\",\n",
    "\n",
    "    \"Location\": \"Location\",\n",
    "\n",
    "    \"Description\": \"Description\",\n",
    "    \"Desc\": \"Description\",\n",
    "    \"Details\": \"Description\",\n",
    "}\n",
    "\n",
    "def merge_labels(df, label_col=\"label\", mapping=None, default_to_other=True):\n",
    "    df = df.copy()\n",
    "    mapping = mapping or {}\n",
    "\n",
    "    def _map(x):\n",
    "        x = str(x)\n",
    "        if x in mapping:\n",
    "            return mapping[x]\n",
    "        return \"Other\" if default_to_other else x\n",
    "\n",
    "    df[label_col] = df[label_col].map(_map)\n",
    "    return df\n",
    "\n",
    "df = merge_labels(df, \"label\", LABEL_MERGE_MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7a4d439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages: 15\n",
      "Total nodes: 2764\n",
      "Unique events: 177\n",
      "Start positives: 177\n",
      "Start rate: 0.06403762662807526\n",
      "Label counts:\n",
      " label\n",
      "Other          1976\n",
      "Date            314\n",
      "Name            150\n",
      "Time            146\n",
      "Location        121\n",
      "Description      57\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = df.sort_values([\"source\", \"rendering_order\"]).reset_index(drop=True)\n",
    "\n",
    "df[\"in_event\"] = df[\"event_id\"].notna().astype(int)\n",
    "df[\"start_event\"] = 0\n",
    "\n",
    "m = df[\"event_id\"].notna()\n",
    "first_idx = df.loc[m].groupby([\"source\", \"event_id\"], sort=False).head(1).index\n",
    "df.loc[first_idx, \"start_event\"] = 1\n",
    "\n",
    "# BIO: 0=O, 1=B, 2=I\n",
    "# B = start of each event record\n",
    "df[\"bio\"] = 0\n",
    "df.loc[df[\"in_event\"].eq(1), \"bio\"] = 2\n",
    "df.loc[df[\"start_event\"].eq(1), \"bio\"] = 1\n",
    "\n",
    "print(\"Pages:\", df[\"source\"].nunique())\n",
    "print(\"Total nodes:\", len(df))\n",
    "print(\"Unique events:\", df.loc[m].drop_duplicates([\"source\",\"event_id\"]).shape[0])\n",
    "print(\"Start positives:\", int(df[\"start_event\"].sum()))\n",
    "print(\"Start rate:\", float(df[\"start_event\"].mean()))\n",
    "print(\"Label counts:\\n\", df[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "215e3a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DEDUP] Dropping duplicate pages:\n",
      "  - members.sacac.org_pattern_labeled\n",
      "\n",
      "After dedup:\n",
      "Pages: 14\n",
      "Total nodes: 2491\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 3) Deduplicate identical pages BEFORE splitting (prevents leakage / misleading CV)\n",
    "# =============================================================================\n",
    "def page_fingerprint(g: pd.DataFrame) -> str:\n",
    "    # Strong-ish fingerprint using ordered sequence of (tag, parent_tag, text_context)\n",
    "    # You can add more columns if you want.\n",
    "    parts = (\n",
    "        g[\"tag\"].astype(str).fillna(\"\") + \"\\t\" +\n",
    "        g[\"parent_tag\"].astype(str).fillna(\"\") + \"\\t\" +\n",
    "        g[\"text_context\"].astype(str).fillna(\"\")\n",
    "    ).tolist()\n",
    "    s = \"\\n\".join(parts)\n",
    "    return hashlib.md5(s.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "fps = []\n",
    "for src, g in df.groupby(\"source\", sort=False):\n",
    "    g = g.sort_values(\"rendering_order\")\n",
    "    fps.append((src, page_fingerprint(g)))\n",
    "\n",
    "fp_df = pd.DataFrame(fps, columns=[\"source\", \"fp\"])\n",
    "dup_groups = fp_df.groupby(\"fp\")[\"source\"].apply(list)\n",
    "\n",
    "# Keep the first source per fingerprint, drop others\n",
    "keep_sources = []\n",
    "drop_sources = []\n",
    "for fp, sources in dup_groups.items():\n",
    "    keep_sources.append(sources[0])\n",
    "    if len(sources) > 1:\n",
    "        drop_sources.extend(sources[1:])\n",
    "\n",
    "if drop_sources:\n",
    "    print(\"\\n[DEDUP] Dropping duplicate pages:\")\n",
    "    for s in drop_sources:\n",
    "        print(\"  -\", s)\n",
    "\n",
    "df = df[df[\"source\"].isin(set(keep_sources))].copy()\n",
    "df = df.sort_values([\"source\", \"rendering_order\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nAfter dedup:\")\n",
    "print(\"Pages:\", df[\"source\"].nunique())\n",
    "print(\"Total nodes:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ceb1c3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "LABELS = sorted(df[\"label\"].unique().tolist())\n",
    "label2id = {l: i for i, l in enumerate(LABELS)}\n",
    "id2label = {i: l for l, i in label2id.items()}\n",
    "OTHER_ID = label2id[\"Other\"]\n",
    "\n",
    "TAG_VOCAB = {t: i for i, t in enumerate(sorted(df[\"tag\"].astype(str).unique().tolist()))}\n",
    "PARENT_TAG_VOCAB = {t: i for i, t in enumerate(sorted(df[\"parent_tag\"].astype(str).unique().tolist()))}\n",
    "\n",
    "STRUCT_COLS_NUM = [\n",
    "    \"depth\",\"sibling_index\",\"children_count\",\"same_tag_sibling_count\",\n",
    "    \"same_text_sibling_count\",\"text_length\",\"word_count\",\n",
    "    \"letter_ratio\",\"digit_ratio\",\"whitespace_ratio\",\"attribute_count\"\n",
    "]\n",
    "STRUCT_COLS_BOOL = [\n",
    "    \"has_link\",\"link_is_absolute\",\"parent_has_link\",\"is_leaf\",\n",
    "    \"contains_date\",\"contains_time\",\"starts_with_digit\",\"ends_with_digit\",\n",
    "    \"has_class\",\"has_id\",\n",
    "    \"attr_has_word_name\",\"attr_has_word_date\",\"attr_has_word_time\",\"attr_has_word_location\",\"attr_has_word_link\",\n",
    "    \"text_has_word_name\",\"text_has_word_date\",\"text_word_time\",\"text_word_description\",\"text_word_location\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf013315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Holdout TEST pages: 2 [np.str_('nacacnet.org_pattern_labeled'), np.str_('neacac_fall.net_pattern_labeled')]\n"
     ]
    }
   ],
   "source": [
    "all_sources = np.array(sorted(df[\"source\"].unique()))\n",
    "rng = np.random.default_rng(42)\n",
    "rng.shuffle(all_sources)\n",
    "\n",
    "TEST_N_PAGES = min(2, len(all_sources))\n",
    "test_sources = set(all_sources[:TEST_N_PAGES])\n",
    "cv_sources   = all_sources[TEST_N_PAGES:]\n",
    "\n",
    "print(\"\\nHoldout TEST pages:\", len(test_sources), sorted(list(test_sources)))\n",
    "test_df = df[df[\"source\"].isin(test_sources)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "488ea22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 6) Normalization: train-only stats per fold\n",
    "# =============================================================================\n",
    "def compute_num_stats(train_df: pd.DataFrame, cols):\n",
    "    x = train_df[cols].fillna(0).values.astype(\"float32\")\n",
    "    mean = x.mean(axis=0)\n",
    "    std = x.std(axis=0)\n",
    "    std = np.where(std < 1e-6, 1.0, std).astype(\"float32\")\n",
    "    return mean.astype(\"float32\"), std.astype(\"float32\")\n",
    "\n",
    "# =============================================================================\n",
    "# 7) Dataset (stores BIO + in_event) and applies normalization\n",
    "# =============================================================================\n",
    "class PageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Caches tokenization as Python lists.\n",
    "    Applies numeric normalization using provided (mean, std).\n",
    "    \"\"\"\n",
    "    def __init__(self, df, tokenizer, num_mean=None, num_std=None, max_tokens=64):\n",
    "        self.pages = []\n",
    "        self.max_tokens = max_tokens\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        if num_mean is None:\n",
    "            num_mean = np.zeros((len(STRUCT_COLS_NUM),), dtype=\"float32\")\n",
    "        if num_std is None:\n",
    "            num_std = np.ones((len(STRUCT_COLS_NUM),), dtype=\"float32\")\n",
    "        self.num_mean = num_mean\n",
    "        self.num_std = num_std\n",
    "\n",
    "        for src, g in df.groupby(\"source\", sort=False):\n",
    "            g = g.sort_values(\"rendering_order\").reset_index(drop=True)\n",
    "\n",
    "            texts = g[\"text_context\"].astype(str).tolist()\n",
    "            enc = tokenizer(\n",
    "                texts,\n",
    "                padding=False,\n",
    "                truncation=True,\n",
    "                max_length=max_tokens,\n",
    "                return_attention_mask=True,\n",
    "                return_tensors=None\n",
    "            )\n",
    "\n",
    "            num = g[STRUCT_COLS_NUM].fillna(0).values.astype(\"float32\")\n",
    "            num = (num - self.num_mean) / self.num_std\n",
    "\n",
    "            page = {\n",
    "                \"input_ids\": enc[\"input_ids\"],\n",
    "                \"attention_mask\": enc[\"attention_mask\"],\n",
    "                \"field_y\": [label2id[x] for x in g[\"label\"].tolist()],\n",
    "                \"bio_y\": g[\"bio\"].astype(int).tolist(),          # 0/1/2\n",
    "                \"in_event_y\": g[\"in_event\"].astype(int).tolist(),# 0/1\n",
    "                \"tag_id\": [TAG_VOCAB[str(x)] for x in g[\"tag\"]],\n",
    "                \"parent_tag_id\": [PARENT_TAG_VOCAB[str(x)] for x in g[\"parent_tag\"]],\n",
    "                \"num_feats\": num,\n",
    "                \"bool_feats\": g[STRUCT_COLS_BOOL].astype(int).values.astype(\"float32\"),\n",
    "            }\n",
    "            self.pages.append(page)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pages)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.pages[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "faf8eb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    B = len(batch)\n",
    "    max_nodes = max(len(x[\"input_ids\"]) for x in batch)\n",
    "\n",
    "    flat = []\n",
    "    node_offsets = []\n",
    "    node_mask = torch.zeros((B, max_nodes), dtype=torch.bool)\n",
    "\n",
    "    start = 0\n",
    "    for i, item in enumerate(batch):\n",
    "        n = len(item[\"input_ids\"])\n",
    "        node_mask[i, :n] = True\n",
    "        for j in range(n):\n",
    "            flat.append({\"input_ids\": item[\"input_ids\"][j], \"attention_mask\": item[\"attention_mask\"][j]})\n",
    "        end = start + n\n",
    "        node_offsets.append((start, end))\n",
    "        start = end\n",
    "\n",
    "    enc = tokenizer.pad(flat, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "    def pad_1d_list(list_of_lists, pad_value, dtype):\n",
    "        out = torch.full((B, max_nodes), pad_value, dtype=dtype)\n",
    "        for i, lst in enumerate(list_of_lists):\n",
    "            n = len(lst)\n",
    "            out[i, :n] = torch.tensor(lst, dtype=dtype)\n",
    "        return out\n",
    "\n",
    "    def pad_2d_array(list_of_arrays, feat_dim, pad_value=0.0, dtype=torch.float32):\n",
    "        out = torch.full((B, max_nodes, feat_dim), pad_value, dtype=dtype)\n",
    "        for i, arr in enumerate(list_of_arrays):\n",
    "            n = arr.shape[0]\n",
    "            out[i, :n, :] = torch.tensor(arr, dtype=dtype)\n",
    "        return out\n",
    "\n",
    "    field_y    = pad_1d_list([x[\"field_y\"] for x in batch], pad_value=-100, dtype=torch.long)\n",
    "    bio_y      = pad_1d_list([x[\"bio_y\"] for x in batch],   pad_value=-100, dtype=torch.long)\n",
    "    in_event_y = pad_1d_list([x[\"in_event_y\"] for x in batch], pad_value=-100, dtype=torch.long)\n",
    "\n",
    "    tag_id        = pad_1d_list([x[\"tag_id\"] for x in batch], pad_value=0, dtype=torch.long)\n",
    "    parent_tag_id = pad_1d_list([x[\"parent_tag_id\"] for x in batch], pad_value=0, dtype=torch.long)\n",
    "\n",
    "    num_feats  = pad_2d_array([x[\"num_feats\"] for x in batch], feat_dim=len(STRUCT_COLS_NUM), pad_value=0.0)\n",
    "    bool_feats = pad_2d_array([x[\"bool_feats\"] for x in batch], feat_dim=len(STRUCT_COLS_BOOL), pad_value=0.0)\n",
    "\n",
    "    return {\n",
    "        \"enc\": enc,\n",
    "        \"node_offsets\": node_offsets,\n",
    "        \"node_mask\": node_mask,\n",
    "        \"field_y\": field_y,\n",
    "        \"bio_y\": bio_y,\n",
    "        \"in_event_y\": in_event_y,\n",
    "        \"tag_id\": tag_id,\n",
    "        \"parent_tag_id\": parent_tag_id,\n",
    "        \"num_feats\": num_feats,\n",
    "        \"bool_feats\": bool_feats,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dfc1ef07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DOMAwareEventExtractor(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        text_model_name: str,\n",
    "        num_field_labels: int,\n",
    "        tag_vocab_size: int,\n",
    "        parent_tag_vocab_size: int,\n",
    "        d_model: int = 128,\n",
    "        nhead: int = 4,\n",
    "        num_layers: int = 2,\n",
    "        dropout: float = 0.2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.text_encoder = AutoModel.from_pretrained(text_model_name)\n",
    "        text_dim = self.text_encoder.config.hidden_size\n",
    "\n",
    "        self.text_proj = nn.Linear(text_dim, d_model)\n",
    "\n",
    "        self.tag_emb = nn.Embedding(tag_vocab_size, d_model)\n",
    "        self.parent_tag_emb = nn.Embedding(parent_tag_vocab_size, d_model)\n",
    "\n",
    "        self.num_proj = nn.Linear(len(STRUCT_COLS_NUM), d_model)\n",
    "        self.bool_proj = nn.Linear(len(STRUCT_COLS_BOOL), d_model)\n",
    "\n",
    "        self.layernorm = nn.LayerNorm(d_model)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.node_encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
    "\n",
    "        self.field_head = nn.Linear(d_model, num_field_labels)\n",
    "        self.bio_head = nn.Linear(d_model, 3)       # O/B/I\n",
    "        self.in_event_head = nn.Linear(d_model, 1)  # binary\n",
    "\n",
    "    def forward(self, enc, node_offsets, node_mask, tag_id, parent_tag_id, num_feats, bool_feats):\n",
    "        out = self.text_encoder(**enc)\n",
    "        cls = out.last_hidden_state[:, 0, :]     # [total_nodes, text_dim]\n",
    "        node_text = self.text_proj(cls)          # [total_nodes, d_model]\n",
    "\n",
    "        B, max_nodes = node_mask.shape\n",
    "        packed = node_text.new_zeros((B, max_nodes, node_text.shape[-1]))\n",
    "        for i, (s, e) in enumerate(node_offsets):\n",
    "            packed[i, : (e - s), :] = node_text[s:e]\n",
    "\n",
    "        x = (\n",
    "            packed\n",
    "            + self.tag_emb(tag_id)\n",
    "            + self.parent_tag_emb(parent_tag_id)\n",
    "            + self.num_proj(num_feats)\n",
    "            + self.bool_proj(bool_feats)\n",
    "        )\n",
    "        x = self.layernorm(x)\n",
    "\n",
    "        key_padding_mask = ~node_mask\n",
    "        x = self.node_encoder(x, src_key_padding_mask=key_padding_mask)\n",
    "\n",
    "        field_logits = self.field_head(x)                  # [B, N, C]\n",
    "        bio_logits   = self.bio_head(x)                    # [B, N, 3]\n",
    "        in_event_logits = self.in_event_head(x).squeeze(-1)# [B, N]\n",
    "        return field_logits, bio_logits, in_event_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1045cb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_losses_for_train_df(\n",
    "    train_df,\n",
    "    LABELS,\n",
    "    device,\n",
    "    other_scale=0.05,\n",
    "    weight_cap=50.0\n",
    "):\n",
    "    # ---- Field weights ----\n",
    "    label_counts = Counter(train_df[\"label\"].tolist())\n",
    "    total = sum(label_counts.values())\n",
    "    weights = []\n",
    "    for label in LABELS:\n",
    "        c = label_counts.get(label, 1)\n",
    "        weights.append(total / c)\n",
    "    weights = torch.tensor(weights, dtype=torch.float32, device=device)\n",
    "    weights = torch.clamp(weights, max=weight_cap)\n",
    "    if \"Other\" in LABELS:\n",
    "        weights[LABELS.index(\"Other\")] *= other_scale\n",
    "\n",
    "    field_loss_fn = nn.CrossEntropyLoss(weight=weights, ignore_index=-100)\n",
    "\n",
    "    # ---- BIO weights ----\n",
    "    bio_counts = Counter(train_df[\"bio\"].tolist())\n",
    "    # bio labels are 0/1/2, but ensure all exist\n",
    "    bio_total = sum(bio_counts.get(i, 0) for i in [0,1,2])\n",
    "    bio_w = []\n",
    "    for i in [0,1,2]:\n",
    "        c = bio_counts.get(i, 1)\n",
    "        bio_w.append(bio_total / c)\n",
    "    bio_w = torch.tensor(bio_w, dtype=torch.float32, device=device)\n",
    "    bio_w = torch.clamp(bio_w, max=weight_cap)\n",
    "    bio_loss_fn = nn.CrossEntropyLoss(weight=bio_w, ignore_index=-100)\n",
    "\n",
    "    # ---- in_event pos_weight ----\n",
    "    pos = float(train_df[\"in_event\"].sum())\n",
    "    neg = float(len(train_df) - pos)\n",
    "    pos_weight = torch.tensor([neg / (pos + 1e-6)], dtype=torch.float32, device=device)\n",
    "    in_event_loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "    return field_loss_fn, bio_loss_fn, in_event_loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a01fb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_starts_from_probs(probs, threshold=0.5, nms_k=1, min_gap=2):\n",
    "    \"\"\"\n",
    "    probs: 1D numpy array of prob(B) per node (valid nodes only)\n",
    "    Returns sorted list of predicted start indices (within valid-node indexing)\n",
    "    Steps:\n",
    "      - threshold\n",
    "      - local maxima (nms_k neighborhood)\n",
    "      - greedy keep with min_gap\n",
    "    \"\"\"\n",
    "    probs = np.asarray(probs)\n",
    "    N = probs.shape[0]\n",
    "    if N == 0:\n",
    "        return []\n",
    "\n",
    "    # Candidates above threshold\n",
    "    cand = np.where(probs >= threshold)[0]\n",
    "    if cand.size == 0:\n",
    "        return []\n",
    "\n",
    "    # Local maxima within +/- nms_k\n",
    "    if nms_k > 0:\n",
    "        keep = []\n",
    "        for i in cand:\n",
    "            lo = max(0, i - nms_k)\n",
    "            hi = min(N, i + nms_k + 1)\n",
    "            if probs[i] >= probs[lo:hi].max() - 1e-12:\n",
    "                keep.append(i)\n",
    "        cand = np.array(keep, dtype=int)\n",
    "        if cand.size == 0:\n",
    "            return []\n",
    "\n",
    "    # Greedy by prob descending, enforce min_gap\n",
    "    order = cand[np.argsort(-probs[cand])]\n",
    "    chosen = []\n",
    "    for idx in order:\n",
    "        if all(abs(idx - j) > min_gap for j in chosen):\n",
    "            chosen.append(int(idx))\n",
    "    return sorted(chosen)\n",
    "\n",
    "def start_prf_with_tolerance(true_starts, pred_starts, tol=1):\n",
    "    \"\"\"\n",
    "    true_starts, pred_starts: sorted lists of indices\n",
    "    tolerance: pred counts as TP if within +/- tol of an unmatched true\n",
    "    \"\"\"\n",
    "    true_starts = list(true_starts)\n",
    "    pred_starts = list(pred_starts)\n",
    "\n",
    "    matched_true = set()\n",
    "    tp = 0\n",
    "    for p in pred_starts:\n",
    "        best = None\n",
    "        best_dist = None\n",
    "        for ti, t in enumerate(true_starts):\n",
    "            if ti in matched_true:\n",
    "                continue\n",
    "            d = abs(p - t)\n",
    "            if d <= tol and (best_dist is None or d < best_dist):\n",
    "                best = ti\n",
    "                best_dist = d\n",
    "        if best is not None:\n",
    "            matched_true.add(best)\n",
    "            tp += 1\n",
    "\n",
    "    fp = len(pred_starts) - tp\n",
    "    fn = len(true_starts) - tp\n",
    "\n",
    "    prec = tp / (tp + fp + 1e-9)\n",
    "    rec  = tp / (tp + fn + 1e-9)\n",
    "    f1   = 2 * prec * rec / (prec + rec + 1e-9)\n",
    "    return prec, rec, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "24e5a412",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_losses_for_train_df(\n",
    "    train_df,\n",
    "    LABELS,\n",
    "    device,\n",
    "    other_scale=0.05,\n",
    "    weight_cap=50.0\n",
    "):\n",
    "    # ---- Field weights ----\n",
    "    label_counts = Counter(train_df[\"label\"].tolist())\n",
    "    total = sum(label_counts.values())\n",
    "    weights = []\n",
    "    for label in LABELS:\n",
    "        c = label_counts.get(label, 1)\n",
    "        weights.append(total / c)\n",
    "    weights = torch.tensor(weights, dtype=torch.float32, device=device)\n",
    "    weights = torch.clamp(weights, max=weight_cap)\n",
    "    if \"Other\" in LABELS:\n",
    "        weights[LABELS.index(\"Other\")] *= other_scale\n",
    "\n",
    "    field_loss_fn = nn.CrossEntropyLoss(weight=weights, ignore_index=-100)\n",
    "\n",
    "    # ---- BIO weights ----\n",
    "    bio_counts = Counter(train_df[\"bio\"].tolist())\n",
    "    # bio labels are 0/1/2, but ensure all exist\n",
    "    bio_total = sum(bio_counts.get(i, 0) for i in [0,1,2])\n",
    "    bio_w = []\n",
    "    for i in [0,1,2]:\n",
    "        c = bio_counts.get(i, 1)\n",
    "        bio_w.append(bio_total / c)\n",
    "    bio_w = torch.tensor(bio_w, dtype=torch.float32, device=device)\n",
    "    bio_w = torch.clamp(bio_w, max=weight_cap)\n",
    "    bio_loss_fn = nn.CrossEntropyLoss(weight=bio_w, ignore_index=-100)\n",
    "\n",
    "    # ---- in_event pos_weight ----\n",
    "    pos = float(train_df[\"in_event\"].sum())\n",
    "    neg = float(len(train_df) - pos)\n",
    "    pos_weight = torch.tensor([neg / (pos + 1e-6)], dtype=torch.float32, device=device)\n",
    "    in_event_loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "    return field_loss_fn, bio_loss_fn, in_event_loss_fn\n",
    "\n",
    "# =============================================================================\n",
    "# 11) Peak-based decoding + start metrics (NMS/min-gap/tolerance)\n",
    "# =============================================================================\n",
    "def pick_starts_from_probs(probs, threshold=0.5, nms_k=1, min_gap=2):\n",
    "    \"\"\"\n",
    "    probs: 1D numpy array of prob(B) per node (valid nodes only)\n",
    "    Returns sorted list of predicted start indices (within valid-node indexing)\n",
    "    Steps:\n",
    "      - threshold\n",
    "      - local maxima (nms_k neighborhood)\n",
    "      - greedy keep with min_gap\n",
    "    \"\"\"\n",
    "    probs = np.asarray(probs)\n",
    "    N = probs.shape[0]\n",
    "    if N == 0:\n",
    "        return []\n",
    "\n",
    "    # Candidates above threshold\n",
    "    cand = np.where(probs >= threshold)[0]\n",
    "    if cand.size == 0:\n",
    "        return []\n",
    "\n",
    "    # Local maxima within +/- nms_k\n",
    "    if nms_k > 0:\n",
    "        keep = []\n",
    "        for i in cand:\n",
    "            lo = max(0, i - nms_k)\n",
    "            hi = min(N, i + nms_k + 1)\n",
    "            if probs[i] >= probs[lo:hi].max() - 1e-12:\n",
    "                keep.append(i)\n",
    "        cand = np.array(keep, dtype=int)\n",
    "        if cand.size == 0:\n",
    "            return []\n",
    "\n",
    "    # Greedy by prob descending, enforce min_gap\n",
    "    order = cand[np.argsort(-probs[cand])]\n",
    "    chosen = []\n",
    "    for idx in order:\n",
    "        if all(abs(idx - j) > min_gap for j in chosen):\n",
    "            chosen.append(int(idx))\n",
    "    return sorted(chosen)\n",
    "\n",
    "def start_prf_with_tolerance(true_starts, pred_starts, tol=1):\n",
    "    \"\"\"\n",
    "    true_starts, pred_starts: sorted lists of indices\n",
    "    tolerance: pred counts as TP if within +/- tol of an unmatched true\n",
    "    \"\"\"\n",
    "    true_starts = list(true_starts)\n",
    "    pred_starts = list(pred_starts)\n",
    "\n",
    "    matched_true = set()\n",
    "    tp = 0\n",
    "    for p in pred_starts:\n",
    "        best = None\n",
    "        best_dist = None\n",
    "        for ti, t in enumerate(true_starts):\n",
    "            if ti in matched_true:\n",
    "                continue\n",
    "            d = abs(p - t)\n",
    "            if d <= tol and (best_dist is None or d < best_dist):\n",
    "                best = ti\n",
    "                best_dist = d\n",
    "        if best is not None:\n",
    "            matched_true.add(best)\n",
    "            tp += 1\n",
    "\n",
    "    fp = len(pred_starts) - tp\n",
    "    fn = len(true_starts) - tp\n",
    "\n",
    "    prec = tp / (tp + fp + 1e-9)\n",
    "    rec  = tp / (tp + fn + 1e-9)\n",
    "    f1   = 2 * prec * rec / (prec + rec + 1e-9)\n",
    "    return prec, rec, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e4f80074",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def collect_page_probs_and_truth(loader, model, device):\n",
    "    \"\"\"\n",
    "    Returns list of (prob_B_valid, true_start_valid) for each page instance in loader.\n",
    "    prob_B_valid: np array length = #valid nodes\n",
    "    true_start_valid: np array length = #valid nodes, {0,1} where start_event==1 (bio==B)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    out = []\n",
    "    for batch in loader:\n",
    "        enc = {k: v.to(device, non_blocking=True) for k, v in batch[\"enc\"].items()}\n",
    "        node_mask = batch[\"node_mask\"].to(device).bool()\n",
    "\n",
    "        field_logits, bio_logits, in_event_logits = model(\n",
    "            enc=enc,\n",
    "            node_offsets=batch[\"node_offsets\"],\n",
    "            node_mask=node_mask,\n",
    "            tag_id=batch[\"tag_id\"].to(device),\n",
    "            parent_tag_id=batch[\"parent_tag_id\"].to(device),\n",
    "            num_feats=batch[\"num_feats\"].to(device),\n",
    "            bool_feats=batch[\"bool_feats\"].to(device),\n",
    "        )\n",
    "\n",
    "        # prob of B (bio label 1)\n",
    "        prob_B = torch.softmax(bio_logits, dim=-1)[..., 1]  # [B,N]\n",
    "\n",
    "        bio_y = batch[\"bio_y\"].to(device)\n",
    "        true_start = (bio_y == 1).long()\n",
    "\n",
    "        prob_B = prob_B.detach().cpu()\n",
    "        true_start = true_start.detach().cpu()\n",
    "        mask = node_mask.detach().cpu()\n",
    "\n",
    "        B, N = prob_B.shape\n",
    "        for b in range(B):\n",
    "            valid = torch.where(mask[b])[0]\n",
    "            if valid.numel() == 0:\n",
    "                continue\n",
    "            out.append((\n",
    "                prob_B[b, valid].numpy(),\n",
    "                true_start[b, valid].numpy().astype(int)\n",
    "            ))\n",
    "    return out\n",
    "\n",
    "@torch.no_grad()\n",
    "def find_best_threshold_peak(\n",
    "    loader,\n",
    "    model,\n",
    "    device,\n",
    "    thresholds=None,\n",
    "    nms_k=1,\n",
    "    min_gap=2,\n",
    "    tol=1\n",
    "):\n",
    "    if thresholds is None:\n",
    "        thresholds = np.linspace(0.05, 0.95, 19)\n",
    "\n",
    "    pages = collect_page_probs_and_truth(loader, model, device)\n",
    "    if len(pages) == 0:\n",
    "        return 0.5, 0.0\n",
    "\n",
    "    best_th = 0.5\n",
    "    best_f1 = -1.0\n",
    "\n",
    "    for th in thresholds:\n",
    "        ps = []\n",
    "        rs = []\n",
    "        fs = []\n",
    "        # Micro over pages by summing TP/FP/FN (more stable than averaging F1)\n",
    "        TP = FP = FN = 0\n",
    "        for prob_B, true_start in pages:\n",
    "            true_idx = np.where(true_start == 1)[0].tolist()\n",
    "            pred_idx = pick_starts_from_probs(prob_B, threshold=th, nms_k=nms_k, min_gap=min_gap)\n",
    "            p, r, f = start_prf_with_tolerance(true_idx, pred_idx, tol=tol)\n",
    "            # Convert per-page p/r/f to counts for micro:\n",
    "            # We can directly derive counts:\n",
    "            # - tp from tolerance matching (recompute counts)\n",
    "            # We'll approximate by converting:\n",
    "            # tp = p*(tp+fp) etc is messy. Let's recompute counts exactly:\n",
    "            # Do exact matching:\n",
    "            matched_true = set()\n",
    "            tp = 0\n",
    "            for pr in pred_idx:\n",
    "                best = None\n",
    "                best_dist = None\n",
    "                for ti, t in enumerate(true_idx):\n",
    "                    if ti in matched_true:\n",
    "                        continue\n",
    "                    d = abs(pr - t)\n",
    "                    if d <= tol and (best_dist is None or d < best_dist):\n",
    "                        best = ti\n",
    "                        best_dist = d\n",
    "                if best is not None:\n",
    "                    matched_true.add(best)\n",
    "                    tp += 1\n",
    "            fp = len(pred_idx) - tp\n",
    "            fn = len(true_idx) - tp\n",
    "\n",
    "            TP += tp\n",
    "            FP += fp\n",
    "            FN += fn\n",
    "\n",
    "        prec = TP / (TP + FP + 1e-9)\n",
    "        rec  = TP / (TP + FN + 1e-9)\n",
    "        f1   = 2 * prec * rec / (prec + rec + 1e-9)\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_th = float(th)\n",
    "\n",
    "    return best_th, float(best_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bc149ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def boundary_metrics_peak(loader, model, device, threshold, nms_k=1, min_gap=2, tol=1):\n",
    "    pages = collect_page_probs_and_truth(loader, model, device)\n",
    "    if len(pages) == 0:\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "    TP = FP = FN = 0\n",
    "    for prob_B, true_start in pages:\n",
    "        true_idx = np.where(true_start == 1)[0].tolist()\n",
    "        pred_idx = pick_starts_from_probs(prob_B, threshold=threshold, nms_k=nms_k, min_gap=min_gap)\n",
    "\n",
    "        matched_true = set()\n",
    "        tp = 0\n",
    "        for pr in pred_idx:\n",
    "            best = None\n",
    "            best_dist = None\n",
    "            for ti, t in enumerate(true_idx):\n",
    "                if ti in matched_true:\n",
    "                    continue\n",
    "                d = abs(pr - t)\n",
    "                if d <= tol and (best_dist is None or d < best_dist):\n",
    "                    best = ti\n",
    "                    best_dist = d\n",
    "            if best is not None:\n",
    "                matched_true.add(best)\n",
    "                tp += 1\n",
    "        fp = len(pred_idx) - tp\n",
    "        fn = len(true_idx) - tp\n",
    "\n",
    "        TP += tp\n",
    "        FP += fp\n",
    "        FN += fn\n",
    "\n",
    "    prec = TP / (TP + FP + 1e-9)\n",
    "    rec  = TP / (TP + FN + 1e-9)\n",
    "    f1   = 2 * prec * rec / (prec + rec + 1e-9)\n",
    "    return float(prec), float(rec), float(f1)\n",
    "\n",
    "# =============================================================================\n",
    "# 12) Field metrics: ignore Other, ignore padding. (Optionally restrict to in_event)\n",
    "# =============================================================================\n",
    "@torch.no_grad()\n",
    "def field_metrics_fast(loader, model, device, label2id, average=\"micro\", restrict_to_true_in_event=False):\n",
    "    model.eval()\n",
    "    yt, yp = [], []\n",
    "    OTHER_ID = label2id[\"Other\"]\n",
    "\n",
    "    for batch in loader:\n",
    "        enc = {k: v.to(device, non_blocking=True) for k, v in batch[\"enc\"].items()}\n",
    "        node_mask = batch[\"node_mask\"].to(device).bool()\n",
    "        field_y = batch[\"field_y\"].to(device)\n",
    "        in_event_y = batch[\"in_event_y\"].to(device)\n",
    "\n",
    "        field_logits, bio_logits, in_event_logits = model(\n",
    "            enc=enc,\n",
    "            node_offsets=batch[\"node_offsets\"],\n",
    "            node_mask=node_mask,\n",
    "            tag_id=batch[\"tag_id\"].to(device),\n",
    "            parent_tag_id=batch[\"parent_tag_id\"].to(device),\n",
    "            num_feats=batch[\"num_feats\"].to(device),\n",
    "            bool_feats=batch[\"bool_feats\"].to(device),\n",
    "        )\n",
    "        pred = torch.argmax(field_logits, dim=-1)\n",
    "\n",
    "        valid = node_mask & (field_y != -100) & (field_y != OTHER_ID)\n",
    "        if restrict_to_true_in_event:\n",
    "            valid = valid & (in_event_y == 1)\n",
    "\n",
    "        yt.extend(field_y[valid].detach().cpu().tolist())\n",
    "        yp.extend(pred[valid].detach().cpu().tolist())\n",
    "\n",
    "    if len(yt) == 0:\n",
    "        return 0.0\n",
    "    return float(f1_score(yt, yp, average=average, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "20298936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 13) Loaders (pass num stats)\n",
    "# =============================================================================\n",
    "def make_loaders(train_df, val_df, num_mean, num_std, batch_size=2, max_tokens=64):\n",
    "    train_dataset = PageDataset(train_df, tokenizer=tokenizer, num_mean=num_mean, num_std=num_std, max_tokens=max_tokens)\n",
    "    val_dataset   = PageDataset(val_df, tokenizer=tokenizer, num_mean=num_mean, num_std=num_std, max_tokens=max_tokens)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "70de5091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 14) Optim: freeze warmup + differential LRs\n",
    "# =============================================================================\n",
    "def init_model_and_optim(lr_bert=5e-6, lr_other=1e-4, weight_decay=0.01):\n",
    "    model = DOMAwareEventExtractor(\n",
    "        text_model_name=MODEL_NAME,\n",
    "        num_field_labels=len(LABELS),\n",
    "        tag_vocab_size=len(TAG_VOCAB),\n",
    "        parent_tag_vocab_size=len(PARENT_TAG_VOCAB),\n",
    "        d_model=128,\n",
    "        nhead=4,\n",
    "        num_layers=2\n",
    "    ).to(device)\n",
    "\n",
    "    bert_params = []\n",
    "    other_params = []\n",
    "    for n, p in model.named_parameters():\n",
    "        if n.startswith(\"text_encoder.\"):\n",
    "            bert_params.append(p)\n",
    "        else:\n",
    "            other_params.append(p)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        [\n",
    "            {\"params\": bert_params, \"lr\": lr_bert, \"weight_decay\": weight_decay},\n",
    "            {\"params\": other_params, \"lr\": lr_other, \"weight_decay\": weight_decay},\n",
    "        ]\n",
    "    )\n",
    "    return model, optimizer\n",
    "\n",
    "def set_bert_trainable(model, trainable: bool):\n",
    "    for p in model.text_encoder.parameters():\n",
    "        p.requires_grad = trainable\n",
    "\n",
    "# =============================================================================\n",
    "# 15) Train / Eval epoch (losses include Other; BIO + in_event auxiliary)\n",
    "# =============================================================================\n",
    "def run_epoch(\n",
    "    model,\n",
    "    optimizer,\n",
    "    loader,\n",
    "    field_loss_fn,\n",
    "    bio_loss_fn,\n",
    "    in_event_loss_fn,\n",
    "    w_bio=2.0,\n",
    "    w_in_event=1.0,\n",
    "    training=True\n",
    "):\n",
    "    model.train() if training else model.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in loader:\n",
    "        enc = {k: v.to(device, non_blocking=True) for k, v in batch[\"enc\"].items()}\n",
    "        node_mask = batch[\"node_mask\"].to(device).bool()\n",
    "\n",
    "        field_y = batch[\"field_y\"].to(device)\n",
    "        bio_y   = batch[\"bio_y\"].to(device)\n",
    "        in_event_y = batch[\"in_event_y\"].to(device)\n",
    "\n",
    "        with torch.set_grad_enabled(training):\n",
    "            field_logits, bio_logits, in_event_logits = model(\n",
    "                enc=enc,\n",
    "                node_offsets=batch[\"node_offsets\"],\n",
    "                node_mask=node_mask,\n",
    "                tag_id=batch[\"tag_id\"].to(device),\n",
    "                parent_tag_id=batch[\"parent_tag_id\"].to(device),\n",
    "                num_feats=batch[\"num_feats\"].to(device),\n",
    "                bool_feats=batch[\"bool_feats\"].to(device),\n",
    "            )\n",
    "\n",
    "            # Field loss: ALL nodes (including Other), excluding padding\n",
    "            field_mask = node_mask & (field_y != -100)\n",
    "            field_loss = field_loss_fn(field_logits[field_mask], field_y[field_mask])\n",
    "\n",
    "            # BIO loss: excluding padding\n",
    "            bio_mask = node_mask & (bio_y != -100)\n",
    "            bio_loss = bio_loss_fn(bio_logits[bio_mask], bio_y[bio_mask])\n",
    "\n",
    "            # in_event loss: excluding padding\n",
    "            ie_mask = node_mask & (in_event_y != -100)\n",
    "            in_event_loss = in_event_loss_fn(in_event_logits[ie_mask], in_event_y[ie_mask].float())\n",
    "\n",
    "            loss = field_loss + w_bio * bio_loss + w_in_event * in_event_loss\n",
    "\n",
    "            if training:\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "        total_loss += loss.detach().item()\n",
    "\n",
    "    return total_loss / max(1, len(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3495f432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Fold 1/5 =====\n",
      "Train pages: 9 Val pages: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 100/100 [00:00<00:00, 308.75it/s, Materializing param=transformer.layer.5.sa_layer_norm.weight]   \n",
      "\u001b[1mDistilBertModel LOAD REPORT\u001b[0m from: distilbert-base-uncased\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "vocab_projector.bias    | UNEXPECTED |  | \n",
      "vocab_transform.weight  | UNEXPECTED |  | \n",
      "vocab_layer_norm.weight | UNEXPECTED |  | \n",
      "vocab_layer_norm.bias   | UNEXPECTED |  | \n",
      "vocab_transform.bias    | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "/home/tony/DTI5902/RP/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:531: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
      "  output = torch._nested_tensor_from_mask(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05 | tr_loss=3.0877 va_loss=3.4717 best_startF1=0.5000 best_th=0.20\n",
      "Epoch 10 | tr_loss=1.8160 va_loss=2.9478 best_startF1=0.5000 best_th=0.20\n",
      "Epoch 15 | tr_loss=1.0159 va_loss=2.7307 best_startF1=0.6286 best_th=0.20\n",
      "Epoch 20 | tr_loss=0.5482 va_loss=2.9602 best_startF1=0.6667 best_th=0.15\n",
      "Fold 1 | START (peak-based): P=0.5500 R=0.8462 F1=0.6667 (th=0.15, nms_k=1, min_gap=2, tol=±1) | field: microF1=0.7872 macroF1=0.8193\n",
      "\n",
      "===== Fold 2/5 =====\n",
      "Train pages: 9 Val pages: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 100/100 [00:00<00:00, 388.88it/s, Materializing param=transformer.layer.5.sa_layer_norm.weight]   \n",
      "\u001b[1mDistilBertModel LOAD REPORT\u001b[0m from: distilbert-base-uncased\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "vocab_projector.bias    | UNEXPECTED |  | \n",
      "vocab_transform.weight  | UNEXPECTED |  | \n",
      "vocab_layer_norm.weight | UNEXPECTED |  | \n",
      "vocab_layer_norm.bias   | UNEXPECTED |  | \n",
      "vocab_transform.bias    | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05 | tr_loss=3.2468 va_loss=4.5417 best_startF1=0.2727 best_th=0.15\n",
      "Epoch 10 | tr_loss=1.8907 va_loss=3.8869 best_startF1=0.3415 best_th=0.15\n",
      "Epoch 15 | tr_loss=1.1671 va_loss=3.5111 best_startF1=0.5556 best_th=0.30\n",
      "Epoch 20 | tr_loss=0.6552 va_loss=3.5826 best_startF1=0.5556 best_th=0.30\n",
      "Fold 2 | START (peak-based): P=0.6250 R=0.5000 F1=0.5556 (th=0.30, nms_k=1, min_gap=2, tol=±1) | field: microF1=0.5714 macroF1=0.5905\n",
      "\n",
      "===== Fold 3/5 =====\n",
      "Train pages: 10 Val pages: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 100/100 [00:00<00:00, 376.28it/s, Materializing param=transformer.layer.5.sa_layer_norm.weight]   \n",
      "\u001b[1mDistilBertModel LOAD REPORT\u001b[0m from: distilbert-base-uncased\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "vocab_projector.bias    | UNEXPECTED |  | \n",
      "vocab_transform.weight  | UNEXPECTED |  | \n",
      "vocab_layer_norm.weight | UNEXPECTED |  | \n",
      "vocab_layer_norm.bias   | UNEXPECTED |  | \n",
      "vocab_transform.bias    | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05 | tr_loss=3.3755 va_loss=3.1104 best_startF1=0.7692 best_th=0.50\n",
      "Epoch 10 | tr_loss=2.1441 va_loss=2.6599 best_startF1=0.7692 best_th=0.50\n",
      "Epoch 15 | tr_loss=1.2009 va_loss=2.7127 best_startF1=0.7692 best_th=0.50\n",
      "Epoch 20 | tr_loss=0.6288 va_loss=2.9545 best_startF1=0.7692 best_th=0.50\n",
      "Fold 3 | START (peak-based): P=1.0000 R=0.6250 F1=0.7692 (th=0.50, nms_k=1, min_gap=2, tol=±1) | field: microF1=0.3617 macroF1=0.1773\n",
      "\n",
      "===== Fold 4/5 =====\n",
      "Train pages: 10 Val pages: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Loading weights: 100%|██████████| 100/100 [00:00<00:00, 394.87it/s, Materializing param=transformer.layer.5.sa_layer_norm.weight]   \n",
      "\u001b[1mDistilBertModel LOAD REPORT\u001b[0m from: distilbert-base-uncased\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "vocab_projector.bias    | UNEXPECTED |  | \n",
      "vocab_transform.weight  | UNEXPECTED |  | \n",
      "vocab_layer_norm.weight | UNEXPECTED |  | \n",
      "vocab_layer_norm.bias   | UNEXPECTED |  | \n",
      "vocab_transform.bias    | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05 | tr_loss=3.2640 va_loss=6.2487 best_startF1=0.2571 best_th=0.45\n",
      "Epoch 10 | tr_loss=1.9724 va_loss=7.1554 best_startF1=0.2571 best_th=0.45\n",
      "Epoch 15 | tr_loss=1.1729 va_loss=8.1777 best_startF1=0.2571 best_th=0.45\n",
      "Epoch 20 | tr_loss=0.6589 va_loss=9.6351 best_startF1=0.2571 best_th=0.45\n",
      "Fold 4 | START (peak-based): P=0.4737 R=0.1765 F1=0.2571 (th=0.45, nms_k=1, min_gap=2, tol=±1) | field: microF1=0.1769 macroF1=0.1371\n",
      "\n",
      "===== Fold 5/5 =====\n",
      "Train pages: 10 Val pages: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 100/100 [00:00<00:00, 376.97it/s, Materializing param=transformer.layer.5.sa_layer_norm.weight]   \n",
      "\u001b[1mDistilBertModel LOAD REPORT\u001b[0m from: distilbert-base-uncased\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "vocab_projector.bias    | UNEXPECTED |  | \n",
      "vocab_transform.weight  | UNEXPECTED |  | \n",
      "vocab_layer_norm.weight | UNEXPECTED |  | \n",
      "vocab_layer_norm.bias   | UNEXPECTED |  | \n",
      "vocab_transform.bias    | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05 | tr_loss=3.5180 va_loss=4.7029 best_startF1=0.8788 best_th=0.45\n",
      "Epoch 10 | tr_loss=2.0957 va_loss=4.6493 best_startF1=0.8788 best_th=0.45\n",
      "Epoch 15 | tr_loss=1.2147 va_loss=4.4498 best_startF1=0.8788 best_th=0.45\n",
      "Epoch 20 | tr_loss=0.6494 va_loss=4.6067 best_startF1=0.8788 best_th=0.45\n",
      "Fold 5 | START (peak-based): P=0.8788 R=0.8788 F1=0.8788 (th=0.45, nms_k=1, min_gap=2, tol=±1) | field: microF1=0.3646 macroF1=0.3083\n",
      "\n",
      "===== CV Summary =====\n",
      "START (peak) Mean F1: 0.6255\n",
      "START (peak) Mean P : 0.7055\n",
      "START (peak) Mean R : 0.6053\n",
      "START (peak) Mean th: 0.3700\n",
      "Field Mean micro-F1 (ignore Other): 0.4524\n",
      "Field Mean macro-F1 (ignore Other): 0.4065\n",
      "Using CV-avg threshold: 0.37\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 16) Cross-val by source\n",
    "# =============================================================================\n",
    "N_SPLITS = min(5, len(cv_sources))\n",
    "kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "\n",
    "# Peak-decoding hyperparams (tune these a bit if needed)\n",
    "NMS_K = 1\n",
    "MIN_GAP = 2\n",
    "TOL = 1\n",
    "\n",
    "EPOCHS = 20\n",
    "FREEZE_EPOCHS = 4  # freeze DistilBERT for first few epochs\n",
    "\n",
    "cv_results = []\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(kf.split(cv_sources), start=1):\n",
    "    fold_train_sources = set(cv_sources[tr_idx])\n",
    "    fold_val_sources   = set(cv_sources[va_idx])\n",
    "\n",
    "    fold_train_df = df[df[\"source\"].isin(fold_train_sources)].copy()\n",
    "    fold_val_df   = df[df[\"source\"].isin(fold_val_sources)].copy()\n",
    "\n",
    "    num_mean, num_std = compute_num_stats(fold_train_df, STRUCT_COLS_NUM)\n",
    "\n",
    "    field_loss_fn, bio_loss_fn, in_event_loss_fn = make_losses_for_train_df(\n",
    "        fold_train_df,\n",
    "        LABELS,\n",
    "        device,\n",
    "        other_scale=0.01,   # downweight Other strongly\n",
    "        weight_cap=50.0\n",
    "    )\n",
    "\n",
    "    print(f\"\\n===== Fold {fold}/{N_SPLITS} =====\")\n",
    "    print(\"Train pages:\", fold_train_df[\"source\"].nunique(), \"Val pages:\", fold_val_df[\"source\"].nunique())\n",
    "\n",
    "    train_loader, val_loader = make_loaders(\n",
    "        fold_train_df, fold_val_df,\n",
    "        num_mean=num_mean, num_std=num_std,\n",
    "        batch_size=2, max_tokens=64\n",
    "    )\n",
    "\n",
    "    model, optimizer = init_model_and_optim(lr_bert=5e-6, lr_other=1e-4)\n",
    "\n",
    "    best = {\"f1\": -1.0, \"th\": 0.5, \"state\": None}\n",
    "\n",
    "    # Freeze BERT initially\n",
    "    set_bert_trainable(model, False)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        if epoch == FREEZE_EPOCHS:\n",
    "            set_bert_trainable(model, True)\n",
    "\n",
    "        tr_loss = run_epoch(\n",
    "            model, optimizer, train_loader,\n",
    "            field_loss_fn, bio_loss_fn, in_event_loss_fn,\n",
    "            w_bio=2.0, w_in_event=1.0,\n",
    "            training=True\n",
    "        )\n",
    "        va_loss = run_epoch(\n",
    "            model, optimizer, val_loader,\n",
    "            field_loss_fn, bio_loss_fn, in_event_loss_fn,\n",
    "            w_bio=2.0, w_in_event=1.0,\n",
    "            training=False\n",
    "        )\n",
    "\n",
    "        th, f1 = find_best_threshold_peak(\n",
    "            val_loader, model, device,\n",
    "            thresholds=np.linspace(0.05, 0.95, 19),\n",
    "            nms_k=NMS_K, min_gap=MIN_GAP, tol=TOL\n",
    "        )\n",
    "\n",
    "        if f1 > best[\"f1\"]:\n",
    "            best[\"f1\"] = f1\n",
    "            best[\"th\"] = th\n",
    "            best[\"state\"] = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1:02d} | tr_loss={tr_loss:.4f} va_loss={va_loss:.4f} best_startF1={best['f1']:.4f} best_th={best['th']:.2f}\")\n",
    "\n",
    "    model.load_state_dict(best[\"state\"])\n",
    "\n",
    "    bp, br, bf1 = boundary_metrics_peak(val_loader, model, device, threshold=best[\"th\"], nms_k=NMS_K, min_gap=MIN_GAP, tol=TOL)\n",
    "\n",
    "    # Field metrics: ignore Other. You can choose restrict_to_true_in_event=True if you prefer.\n",
    "    field_micro_f1 = field_metrics_fast(val_loader, model, device, label2id=label2id, average=\"micro\", restrict_to_true_in_event=False)\n",
    "    field_macro_f1 = field_metrics_fast(val_loader, model, device, label2id=label2id, average=\"macro\", restrict_to_true_in_event=False)\n",
    "\n",
    "    print(\n",
    "        f\"Fold {fold} | START (peak-based): P={bp:.4f} R={br:.4f} F1={bf1:.4f} (th={best['th']:.2f}, nms_k={NMS_K}, min_gap={MIN_GAP}, tol=±{TOL})\"\n",
    "        f\" | field: microF1={field_micro_f1:.4f} macroF1={field_macro_f1:.4f}\"\n",
    "    )\n",
    "\n",
    "    cv_results.append({\n",
    "        \"fold\": fold,\n",
    "        \"bp\": bp, \"br\": br, \"bf1\": bf1, \"th\": best[\"th\"],\n",
    "        \"field_micro_f1\": field_micro_f1,\n",
    "        \"field_macro_f1\": field_macro_f1,\n",
    "        \"num_mean\": num_mean,\n",
    "        \"num_std\": num_std,\n",
    "    })\n",
    "\n",
    "print(\"\\n===== CV Summary =====\")\n",
    "mean_bf1 = float(np.mean([x[\"bf1\"] for x in cv_results])) if cv_results else 0.0\n",
    "mean_bp  = float(np.mean([x[\"bp\"]  for x in cv_results])) if cv_results else 0.0\n",
    "mean_br  = float(np.mean([x[\"br\"]  for x in cv_results])) if cv_results else 0.0\n",
    "mean_th  = float(np.mean([x[\"th\"]  for x in cv_results])) if cv_results else 0.5\n",
    "mean_field_micro = float(np.mean([x[\"field_micro_f1\"] for x in cv_results])) if cv_results else 0.0\n",
    "mean_field_macro = float(np.mean([x[\"field_macro_f1\"] for x in cv_results])) if cv_results else 0.0\n",
    "\n",
    "print(f\"START (peak) Mean F1: {mean_bf1:.4f}\")\n",
    "print(f\"START (peak) Mean P : {mean_bp:.4f}\")\n",
    "print(f\"START (peak) Mean R : {mean_br:.4f}\")\n",
    "print(f\"START (peak) Mean th: {mean_th:.4f}\")\n",
    "print(f\"Field Mean micro-F1 (ignore Other): {mean_field_micro:.4f}\")\n",
    "print(f\"Field Mean macro-F1 (ignore Other): {mean_field_macro:.4f}\")\n",
    "\n",
    "best_th_cv = mean_th\n",
    "print(\"Using CV-avg threshold:\", best_th_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e34ba10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 100/100 [00:00<00:00, 429.90it/s, Materializing param=transformer.layer.5.sa_layer_norm.weight]   \n",
      "\u001b[1mDistilBertModel LOAD REPORT\u001b[0m from: distilbert-base-uncased\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "vocab_projector.bias    | UNEXPECTED |  | \n",
      "vocab_transform.weight  | UNEXPECTED |  | \n",
      "vocab_layer_norm.weight | UNEXPECTED |  | \n",
      "vocab_layer_norm.bias   | UNEXPECTED |  | \n",
      "vocab_transform.bias    | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FINAL] Epoch 05 | tr_loss=3.1594\n",
      "[FINAL] Epoch 10 | tr_loss=2.0299\n",
      "[FINAL] Epoch 15 | tr_loss=1.1433\n",
      "[FINAL] Epoch 20 | tr_loss=0.6481\n",
      "\n",
      "===== HOLDOUT TEST (START / peak-based) =====\n",
      "Threshold=0.37  P=0.5000  R=0.2812  F1=0.3600 (nms_k=1, min_gap=2, tol=±1)\n",
      "\n",
      "===== HOLDOUT TEST (FIELD / ignore Other) =====\n",
      "microF1=0.9073 macroF1=0.5762\n",
      "\n",
      "Holdout test sources: [np.str_('nacacnet.org_pattern_labeled'), np.str_('neacac_fall.net_pattern_labeled')]\n",
      "Holdout start positives (total): 32\n",
      "Holdout start positives per page:\n",
      " source\n",
      "nacacnet.org_pattern_labeled       22\n",
      "neacac_fall.net_pattern_labeled    10\n",
      "Name: start_event, dtype: int64\n",
      "\n",
      "[DEBUG] Best holdout threshold (peak metric): 0.05 Best holdout F1: 0.7341772146893126\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 17) Final training on ALL CV pages, evaluate on HOLDOUT TEST\n",
    "# =============================================================================\n",
    "final_train_df = df[df[\"source\"].isin(set(cv_sources))].copy()\n",
    "\n",
    "# Normalize using ALL final-train stats\n",
    "final_num_mean, final_num_std = compute_num_stats(final_train_df, STRUCT_COLS_NUM)\n",
    "\n",
    "final_train_loader, _ = make_loaders(\n",
    "    final_train_df, final_train_df,\n",
    "    num_mean=final_num_mean, num_std=final_num_std,\n",
    "    batch_size=2, max_tokens=64\n",
    ")\n",
    "\n",
    "test_dataset = PageDataset(test_df, tokenizer=tokenizer, num_mean=final_num_mean, num_std=final_num_std, max_tokens=64)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "field_loss_fn, bio_loss_fn, in_event_loss_fn = make_losses_for_train_df(\n",
    "    final_train_df, LABELS, device, other_scale=0.01, weight_cap=50.0\n",
    ")\n",
    "\n",
    "final_model, final_optimizer = init_model_and_optim(lr_bert=5e-6, lr_other=1e-4)\n",
    "\n",
    "# Freeze then unfreeze\n",
    "set_bert_trainable(final_model, False)\n",
    "\n",
    "EPOCHS_FINAL = 20\n",
    "for epoch in range(EPOCHS_FINAL):\n",
    "    if epoch == FREEZE_EPOCHS:\n",
    "        set_bert_trainable(final_model, True)\n",
    "\n",
    "    tr_loss = run_epoch(\n",
    "        final_model, final_optimizer, final_train_loader,\n",
    "        field_loss_fn, bio_loss_fn, in_event_loss_fn,\n",
    "        w_bio=2.0, w_in_event=1.0,\n",
    "        training=True\n",
    "    )\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"[FINAL] Epoch {epoch+1:02d} | tr_loss={tr_loss:.4f}\")\n",
    "\n",
    "# Evaluate peak-based start detection on HOLDOUT\n",
    "p, r, f1 = boundary_metrics_peak(test_loader, final_model, device, threshold=best_th_cv, nms_k=NMS_K, min_gap=MIN_GAP, tol=TOL)\n",
    "print(\"\\n===== HOLDOUT TEST (START / peak-based) =====\")\n",
    "print(f\"Threshold={best_th_cv:.2f}  P={p:.4f}  R={r:.4f}  F1={f1:.4f} (nms_k={NMS_K}, min_gap={MIN_GAP}, tol=±{TOL})\")\n",
    "\n",
    "# Field metrics on HOLDOUT (ignore Other)\n",
    "field_micro = field_metrics_fast(test_loader, final_model, device, label2id, average=\"micro\", restrict_to_true_in_event=False)\n",
    "field_macro = field_metrics_fast(test_loader, final_model, device, label2id, average=\"macro\", restrict_to_true_in_event=False)\n",
    "print(\"\\n===== HOLDOUT TEST (FIELD / ignore Other) =====\")\n",
    "print(f\"microF1={field_micro:.4f} macroF1={field_macro:.4f}\")\n",
    "\n",
    "print(\"\\nHoldout test sources:\", sorted(list(test_sources)))\n",
    "print(\"Holdout start positives (total):\", int(test_df[\"start_event\"].sum()))\n",
    "per_page = test_df.groupby(\"source\")[\"start_event\"].sum().sort_values(ascending=False)\n",
    "print(\"Holdout start positives per page:\\n\", per_page.astype(int))\n",
    "\n",
    "# Optionally tune threshold on holdout (NOT for reporting, just for debugging)\n",
    "best_th_test, best_f1_test = find_best_threshold_peak(test_loader, final_model, device, nms_k=NMS_K, min_gap=MIN_GAP, tol=TOL)\n",
    "print(\"\\n[DEBUG] Best holdout threshold (peak metric):\", best_th_test, \"Best holdout F1:\", best_f1_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
