{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e833499f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tony/DTI5902/RP/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rendering_order</th>\n",
       "      <th>depth</th>\n",
       "      <th>parent_index</th>\n",
       "      <th>text_length</th>\n",
       "      <th>sibling_index</th>\n",
       "      <th>children_count</th>\n",
       "      <th>same_tag_sibling_count</th>\n",
       "      <th>same_text_sibling_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>letter_ratio</th>\n",
       "      <th>digit_ratio</th>\n",
       "      <th>whitespace_ratio</th>\n",
       "      <th>attribute_count</th>\n",
       "      <th>event_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2764.000000</td>\n",
       "      <td>2764.000000</td>\n",
       "      <td>2764.000000</td>\n",
       "      <td>2764.000000</td>\n",
       "      <td>2764.000000</td>\n",
       "      <td>2764.000000</td>\n",
       "      <td>2764.000000</td>\n",
       "      <td>2764.000000</td>\n",
       "      <td>2764.000000</td>\n",
       "      <td>2764.000000</td>\n",
       "      <td>2764.00000</td>\n",
       "      <td>2764.000000</td>\n",
       "      <td>2764.000000</td>\n",
       "      <td>789.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>665.224674</td>\n",
       "      <td>17.407742</td>\n",
       "      <td>659.378075</td>\n",
       "      <td>26.265557</td>\n",
       "      <td>0.778220</td>\n",
       "      <td>0.140376</td>\n",
       "      <td>1.033285</td>\n",
       "      <td>0.001447</td>\n",
       "      <td>4.020984</td>\n",
       "      <td>0.767786</td>\n",
       "      <td>0.12120</td>\n",
       "      <td>0.075350</td>\n",
       "      <td>0.921129</td>\n",
       "      <td>11.693283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>528.325947</td>\n",
       "      <td>6.531681</td>\n",
       "      <td>526.280103</td>\n",
       "      <td>43.190741</td>\n",
       "      <td>2.411446</td>\n",
       "      <td>0.520146</td>\n",
       "      <td>3.663405</td>\n",
       "      <td>0.038021</td>\n",
       "      <td>6.543288</td>\n",
       "      <td>0.302293</td>\n",
       "      <td>0.27128</td>\n",
       "      <td>0.061539</td>\n",
       "      <td>1.733493</td>\n",
       "      <td>9.542569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>314.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>307.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>514.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>507.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.885714</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>837.500000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>833.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2375.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>2366.000000</td>\n",
       "      <td>559.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>42.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       rendering_order        depth  parent_index  text_length  sibling_index  \\\n",
       "count      2764.000000  2764.000000   2764.000000  2764.000000    2764.000000   \n",
       "mean        665.224674    17.407742    659.378075    26.265557       0.778220   \n",
       "std         528.325947     6.531681    526.280103    43.190741       2.411446   \n",
       "min          19.000000     2.000000     18.000000     1.000000       0.000000   \n",
       "25%         314.000000    12.000000    307.000000     8.000000       0.000000   \n",
       "50%         514.000000    17.000000    507.000000    16.000000       0.000000   \n",
       "75%         837.500000    23.000000    833.000000    27.000000       0.000000   \n",
       "max        2375.000000    30.000000   2366.000000   559.000000      25.000000   \n",
       "\n",
       "       children_count  same_tag_sibling_count  same_text_sibling_count  \\\n",
       "count     2764.000000             2764.000000              2764.000000   \n",
       "mean         0.140376                1.033285                 0.001447   \n",
       "std          0.520146                3.663405                 0.038021   \n",
       "min          0.000000                0.000000                 0.000000   \n",
       "25%          0.000000                0.000000                 0.000000   \n",
       "50%          0.000000                0.000000                 0.000000   \n",
       "75%          0.000000                0.000000                 0.000000   \n",
       "max         11.000000               25.000000                 1.000000   \n",
       "\n",
       "        word_count  letter_ratio  digit_ratio  whitespace_ratio  \\\n",
       "count  2764.000000   2764.000000   2764.00000       2764.000000   \n",
       "mean      4.020984      0.767786      0.12120          0.075350   \n",
       "std       6.543288      0.302293      0.27128          0.061539   \n",
       "min       1.000000      0.000000      0.00000          0.000000   \n",
       "25%       1.000000      0.785714      0.00000          0.000000   \n",
       "50%       2.000000      0.885714      0.00000          0.083333   \n",
       "75%       4.000000      0.941176      0.00000          0.117647   \n",
       "max      81.000000      1.000000      1.00000          0.333333   \n",
       "\n",
       "       attribute_count    event_id  \n",
       "count      2764.000000  789.000000  \n",
       "mean          0.921129   11.693283  \n",
       "std           1.733493    9.542569  \n",
       "min           0.000000    1.000000  \n",
       "25%           0.000000    4.000000  \n",
       "50%           1.000000    8.000000  \n",
       "75%           1.000000   19.000000  \n",
       "max          15.000000   42.000000  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import os\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, f1_score\n",
    "from transformers import AutoTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_path = os.path.join(os.getcwd(), '..', 'data', 'cleaned', 'full_data.csv')\n",
    "df = pd.read_csv(data_path)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e8fab13",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_MERGE_MAP = {\n",
    "    # keep\n",
    "    \"Other\": \"Other\",\n",
    "\n",
    "    # Name variants\n",
    "    \"Name\": \"Name\",\n",
    "    \"NameLink\": \"Name\",\n",
    "    \"NameLocation\": \"Name\",\n",
    "\n",
    "    # Date variants\n",
    "    \"Date\": \"Date\",\n",
    "    \"DateTime\": \"Date\",      \n",
    "\n",
    "    # Time variants\n",
    "    \"Time\": \"Time\",\n",
    "    \"StartTime\": \"Time\",\n",
    "    \"EndTime\": \"Time\",\n",
    "    \"StartEndTime\": \"Time\",\n",
    "    \"TimeLocation\": \"Time\",\n",
    "\n",
    "  \n",
    "    \"Location\": \"Location\",\n",
    "\n",
    "    \"Description\": \"Description\",\n",
    "    \"Desc\": \"Description\",\n",
    "    \"Details\": \"Description\",\n",
    "}\n",
    "\n",
    "def merge_labels(df, label_col=\"label\", mapping=None, default_to_other=True):\n",
    "    df = df.copy()\n",
    "    mapping = mapping or {}\n",
    "\n",
    "    def _map(x):\n",
    "        if x in mapping:\n",
    "            return mapping[x]\n",
    "        return \"Other\" if default_to_other else x\n",
    "\n",
    "    df[label_col] = df[label_col].astype(str).map(_map)\n",
    "    return df\n",
    "\n",
    "df = merge_labels(df, label_col=\"label\", mapping=LABEL_MERGE_MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7a4d439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages: 15\n",
      "Total nodes: 2764\n",
      "start_event positives: 177\n",
      "Label counts:\n",
      " label\n",
      "Other          1976\n",
      "Date            314\n",
      "Name            150\n",
      "Time            146\n",
      "Location        121\n",
      "Description      57\n",
      "Name: count, dtype: int64\n",
      "Start_event positive rate: 0.06403762662807526\n",
      "start_event positives: 177\n",
      "unique events: 177\n"
     ]
    }
   ],
   "source": [
    "# Sort within each page\n",
    "df[\"start_event\"] = 0\n",
    "m = df[\"event_id\"].notna()\n",
    "\n",
    "# Ensure a stable DOM order\n",
    "# If you have rendering_order already: sort by it\n",
    "df = df.sort_values([\"source\", \"rendering_order\"]).reset_index(drop=True)\n",
    "\n",
    "first_idx = df.loc[m].groupby([\"source\", \"event_id\"], sort=False).head(1).index\n",
    "df.loc[first_idx, \"start_event\"] = 1\n",
    "\n",
    "print(\"Pages:\", df[\"source\"].nunique())\n",
    "print(\"Total nodes:\", len(df))\n",
    "print(\"start_event positives:\", int(df[\"start_event\"].sum()))\n",
    "print(\"Label counts:\\n\", df[\"label\"].value_counts())\n",
    "\n",
    "# positive rate overall\n",
    "print(\"Start_event positive rate:\", df[\"start_event\"].mean())\n",
    "\n",
    "m = df[\"event_id\"].notna()\n",
    "print(\"start_event positives:\", df[\"start_event\"].sum())\n",
    "print(\"unique events:\", df.loc[m].drop_duplicates([\"source\",\"event_id\"]).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ceb1c3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holdout TEST pages: 2 [np.str_('members.sacac.org_pattern_labeled'), np.str_('nacacnet.org_pattern_labeled')]\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 1) Choose a fixed TEST set of pages (holdout)\n",
    "# -------------------------------\n",
    "all_sources = np.array(sorted(df[\"source\"].unique()))\n",
    "rng = np.random.default_rng(42)\n",
    "rng.shuffle(all_sources)\n",
    "\n",
    "TEST_N_PAGES = 2\n",
    "test_sources = set(all_sources[:TEST_N_PAGES])\n",
    "cv_sources   = all_sources[TEST_N_PAGES:]   # remaining pages used for cross-val\n",
    "\n",
    "print(\"Holdout TEST pages:\", len(test_sources), sorted(list(test_sources)))\n",
    "\n",
    "test_df = df[df[\"source\"].isin(test_sources)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf013315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "12.8\n"
     ]
    }
   ],
   "source": [
    "#check if cuda is available\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Labels: keep as-is\n",
    "LABELS = sorted(df[\"label\"].unique().tolist())\n",
    "label2id = {l:i for i,l in enumerate(LABELS)}\n",
    "id2label = {i:l for l,i in label2id.items()}\n",
    "OTHER_ID = label2id[\"Other\"]\n",
    "# Vocab for categorical structural features\n",
    "TAG_VOCAB = {t:i for i,t in enumerate(sorted(df[\"tag\"].astype(str).unique().tolist()))}\n",
    "PARENT_TAG_VOCAB = {t:i for i,t in enumerate(sorted(df[\"parent_tag\"].astype(str).unique().tolist()))}\n",
    "\n",
    "# Structural columns (already in your CSV)\n",
    "STRUCT_COLS_NUM = [\n",
    "    \"depth\",\"sibling_index\",\"children_count\",\"same_tag_sibling_count\",\n",
    "    \"same_text_sibling_count\",\"text_length\",\"word_count\",\n",
    "    \"letter_ratio\",\"digit_ratio\",\"whitespace_ratio\",\"attribute_count\"\n",
    "]\n",
    "STRUCT_COLS_BOOL = [\n",
    "    \"has_link\",\"link_is_absolute\",\"parent_has_link\",\"is_leaf\",\n",
    "    \"contains_date\",\"contains_time\",\"starts_with_digit\",\"ends_with_digit\",\n",
    "    \"has_class\",\"has_id\",\n",
    "    \"attr_has_word_name\",\"attr_has_word_date\",\"attr_has_word_time\",\"attr_has_word_location\",\"attr_has_word_link\",\n",
    "    \"text_has_word_name\",\"text_has_word_date\",\"text_word_time\",\"text_word_description\",\"text_word_location\"\n",
    "]\n",
    "\n",
    "class PageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Caches tokenization as Python lists (NOT tensors) to avoid heavy per-batch Python padding.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, tokenizer, max_tokens=64):\n",
    "        self.pages = []\n",
    "        for src, g in df.groupby(\"source\"):\n",
    "            g = g.sort_values(\"rendering_order\").reset_index(drop=True)\n",
    "\n",
    "            texts = g[\"text_context\"].astype(str).tolist()\n",
    "\n",
    "            # tokenize once per page -> list[list[int]]\n",
    "            enc = tokenizer(\n",
    "                texts,\n",
    "                padding=False,\n",
    "                truncation=True,\n",
    "                max_length=max_tokens,\n",
    "                return_attention_mask=True,\n",
    "                return_tensors=None\n",
    "            )\n",
    "\n",
    "            page = {\n",
    "                \"input_ids\": enc[\"input_ids\"],             # list of lists (per node)\n",
    "                \"attention_mask\": enc[\"attention_mask\"],   # list of lists (per node)\n",
    "                \"field_y\": [label2id[x] for x in g[\"label\"].tolist()],\n",
    "                \"boundary_y\": g[\"start_event\"].astype(int).tolist(),\n",
    "                \"tag_id\": [TAG_VOCAB[str(x)] for x in g[\"tag\"]],\n",
    "                \"parent_tag_id\": [PARENT_TAG_VOCAB[str(x)] for x in g[\"parent_tag\"]],\n",
    "                \"num_feats\": g[STRUCT_COLS_NUM].fillna(0).values.astype(\"float32\"),\n",
    "                \"bool_feats\": g[STRUCT_COLS_BOOL].astype(int).values.astype(\"float32\"),\n",
    "            }\n",
    "            self.pages.append(page)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pages)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.pages[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "488ea22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Fast collate:\n",
    "      - pads tokens via tokenizer.pad (vectorized)\n",
    "      - pads node dimension for labels/features\n",
    "    \"\"\"\n",
    "    B = len(batch)\n",
    "    max_nodes = max(len(x[\"input_ids\"]) for x in batch)\n",
    "\n",
    "    # ---- flatten nodes across pages ----\n",
    "    flat = []\n",
    "    node_offsets = []\n",
    "    node_mask = torch.zeros((B, max_nodes), dtype=torch.bool)\n",
    "\n",
    "    start = 0\n",
    "    for i, item in enumerate(batch):\n",
    "        n = len(item[\"input_ids\"])\n",
    "        node_mask[i, :n] = True\n",
    "\n",
    "        for j in range(n):\n",
    "            flat.append({\n",
    "                \"input_ids\": item[\"input_ids\"][j],\n",
    "                \"attention_mask\": item[\"attention_mask\"][j],\n",
    "            })\n",
    "\n",
    "        end = start + n\n",
    "        node_offsets.append((start, end))\n",
    "        start = end\n",
    "\n",
    "    # ---- pad tokens in one shot (FAST) ----\n",
    "    enc = tokenizer.pad(\n",
    "        flat,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )  # dict of tensors [total_nodes, max_seq_len]\n",
    "\n",
    "    # ---- helpers to pad node-dimension tensors ----\n",
    "    def pad_1d_list(list_of_lists, pad_value, dtype):\n",
    "        out = torch.full((B, max_nodes), pad_value, dtype=dtype)\n",
    "        for i, lst in enumerate(list_of_lists):\n",
    "            n = len(lst)\n",
    "            out[i, :n] = torch.tensor(lst, dtype=dtype)\n",
    "        return out\n",
    "\n",
    "    def pad_2d_array(list_of_arrays, feat_dim, pad_value=0.0, dtype=torch.float32):\n",
    "        out = torch.full((B, max_nodes, feat_dim), pad_value, dtype=dtype)\n",
    "        for i, arr in enumerate(list_of_arrays):\n",
    "            n = arr.shape[0]\n",
    "            out[i, :n, :] = torch.tensor(arr, dtype=dtype)\n",
    "        return out\n",
    "\n",
    "    field_y    = pad_1d_list([x[\"field_y\"] for x in batch], pad_value=-100, dtype=torch.long)\n",
    "    boundary_y = pad_1d_list([x[\"boundary_y\"] for x in batch], pad_value=0, dtype=torch.float32)\n",
    "\n",
    "    tag_id        = pad_1d_list([x[\"tag_id\"] for x in batch], pad_value=0, dtype=torch.long)\n",
    "    parent_tag_id = pad_1d_list([x[\"parent_tag_id\"] for x in batch], pad_value=0, dtype=torch.long)\n",
    "\n",
    "    num_feats  = pad_2d_array([x[\"num_feats\"] for x in batch], feat_dim=len(STRUCT_COLS_NUM), pad_value=0.0)\n",
    "    bool_feats = pad_2d_array([x[\"bool_feats\"] for x in batch], feat_dim=len(STRUCT_COLS_BOOL), pad_value=0.0)\n",
    "\n",
    "    return {\n",
    "        \"enc\": enc,\n",
    "        \"node_offsets\": node_offsets,\n",
    "        \"node_mask\": node_mask,\n",
    "        \"field_y\": field_y,\n",
    "        \"boundary_y\": boundary_y,\n",
    "        \"tag_id\": tag_id,\n",
    "        \"parent_tag_id\": parent_tag_id,\n",
    "        \"num_feats\": num_feats,\n",
    "        \"bool_feats\": bool_feats,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "faf8eb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DOMAwareEventExtractor(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        text_model_name: str,\n",
    "        num_field_labels: int,\n",
    "        tag_vocab_size: int,\n",
    "        parent_tag_vocab_size: int,\n",
    "        d_model: int = 128,\n",
    "        nhead: int = 4,\n",
    "        num_layers: int = 2,\n",
    "        dropout: float = 0.2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.text_encoder = AutoModel.from_pretrained(text_model_name)\n",
    "        text_dim = self.text_encoder.config.hidden_size\n",
    "\n",
    "        self.text_proj = nn.Linear(text_dim, d_model)\n",
    "\n",
    "        self.tag_emb = nn.Embedding(tag_vocab_size, d_model)\n",
    "        self.parent_tag_emb = nn.Embedding(parent_tag_vocab_size, d_model)\n",
    "\n",
    "        self.num_proj = nn.Linear(len(STRUCT_COLS_NUM), d_model)\n",
    "        self.bool_proj = nn.Linear(len(STRUCT_COLS_BOOL), d_model)\n",
    "\n",
    "        self.layernorm = nn.LayerNorm(d_model)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.node_encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
    "\n",
    "        self.field_head = nn.Linear(d_model, num_field_labels)\n",
    "        self.boundary_head = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, enc, node_offsets, node_mask, tag_id, parent_tag_id, num_feats, bool_feats):\n",
    "        out = self.text_encoder(**enc)\n",
    "        cls = out.last_hidden_state[:, 0, :]           # [total_nodes, text_dim]\n",
    "        node_text = self.text_proj(cls)                # [total_nodes, d_model]\n",
    "\n",
    "        B, max_nodes = node_mask.shape\n",
    "        packed = node_text.new_zeros((B, max_nodes, node_text.shape[-1]))\n",
    "        for i, (s, e) in enumerate(node_offsets):\n",
    "            packed[i, : (e - s), :] = node_text[s:e]\n",
    "\n",
    "        x = packed \\\n",
    "            + self.tag_emb(tag_id) \\\n",
    "            + self.parent_tag_emb(parent_tag_id) \\\n",
    "            + self.num_proj(num_feats) \\\n",
    "            + self.bool_proj(bool_feats)\n",
    "\n",
    "        x = self.layernorm(x)\n",
    "\n",
    "        key_padding_mask = ~node_mask\n",
    "        x = self.node_encoder(x, src_key_padding_mask=key_padding_mask)\n",
    "\n",
    "        field_logits = self.field_head(x)                    # [B, N, C]\n",
    "        boundary_logits = self.boundary_head(x).squeeze(-1)  # [B, N]\n",
    "        return field_logits, boundary_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfc1ef07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLossWithLogits(nn.Module):\n",
    "    \"\"\"\n",
    "    Binary focal loss operating on logits.\n",
    "    alpha: weight for positive class (0..1). Often 0.25.\n",
    "    gamma: focusing parameter. Often 2.0.\n",
    "    reduction: 'mean' or 'sum'\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=0.9, gamma=2.0, reduction=\"mean\"):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        targets = targets.float()\n",
    "        bce = F.binary_cross_entropy_with_logits(logits, targets, reduction=\"none\")\n",
    "        p = torch.sigmoid(logits)\n",
    "        pt = torch.where(targets == 1, p, 1 - p)          # prob of the true class\n",
    "        alpha_t = torch.where(targets == 1, self.alpha, 1 - self.alpha)\n",
    "        loss = alpha_t * (1 - pt).pow(self.gamma) * bce\n",
    "\n",
    "        if self.reduction == \"mean\":\n",
    "            return loss.mean()\n",
    "        if self.reduction == \"sum\":\n",
    "            return loss.sum()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1045cb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_losses_for_train_df(train_df, LABELS, device, other_scale=0.05, weight_cap=50.0):\n",
    "    \"\"\"\n",
    "    Build loss functions using TRAIN ONLY statistics (correct for CV).\n",
    "    Returns: (field_loss_fn, boundary_loss_fn)\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- Field class weights computed on TRAIN ONLY ----\n",
    "    label_counts = Counter(train_df[\"label\"].tolist())\n",
    "    total = sum(label_counts.values())\n",
    "\n",
    "    weights = []\n",
    "    for label in LABELS:\n",
    "        count = label_counts.get(label, 1)\n",
    "        weights.append(total / count)\n",
    "\n",
    "    weights = torch.tensor(weights, dtype=torch.float32).to(device)\n",
    "    weights = torch.clamp(weights, max=weight_cap)  \n",
    "\n",
    "    if \"Other\" in LABELS: #downweight the other class\n",
    "        other_idx = LABELS.index(\"Other\")\n",
    "        weights[other_idx] *= other_scale\n",
    "\n",
    "    field_loss_fn = nn.CrossEntropyLoss(\n",
    "        weight=weights,\n",
    "        ignore_index=-100\n",
    "    )\n",
    "\n",
    "    # ---- Boundary pos_weight computed on TRAIN ONLY ----\n",
    "    pos = float(train_df[\"start_event\"].sum())\n",
    "    neg = float(len(train_df) - pos)\n",
    "    pos_weight = torch.tensor([neg / (pos + 1e-6)], dtype=torch.float32).to(device)\n",
    "\n",
    "    boundary_loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "    return field_loss_fn, boundary_loss_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a01fb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_on_loader(\n",
    "    loader,\n",
    "    model,\n",
    "    field_loss_fn,\n",
    "    boundary_loss_fn,\n",
    "    LABELS,\n",
    "    id2label,\n",
    "    device,\n",
    "    threshold=0.5,\n",
    "    boundary_weight=3.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Eval for both field + boundary WITHOUT suppression.\n",
    "    Padding is excluded via node_mask in both losses + metrics.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    all_field_true, all_field_pred = [], []\n",
    "    all_bound_true, all_bound_pred = [], []\n",
    "\n",
    "    for batch in loader:\n",
    "        enc = {k: v.to(device, non_blocking=True) for k, v in batch[\"enc\"].items()}\n",
    "        node_mask = batch[\"node_mask\"].to(device).bool()\n",
    "\n",
    "        tag_id = batch[\"tag_id\"].to(device)\n",
    "        parent_tag_id = batch[\"parent_tag_id\"].to(device)\n",
    "        num_feats = batch[\"num_feats\"].to(device)\n",
    "        bool_feats = batch[\"bool_feats\"].to(device)\n",
    "\n",
    "        field_y = batch[\"field_y\"].to(device)\n",
    "        boundary_y = batch[\"boundary_y\"].to(device)\n",
    "\n",
    "        field_logits, boundary_logits = model(\n",
    "            enc=enc,\n",
    "            node_offsets=batch[\"node_offsets\"],\n",
    "            node_mask=node_mask,\n",
    "            tag_id=tag_id,\n",
    "            parent_tag_id=parent_tag_id,\n",
    "            num_feats=num_feats,\n",
    "            bool_feats=bool_feats\n",
    "        )\n",
    "\n",
    "        # ---- losses (match training) ----\n",
    "        field_loss = field_loss_fn(field_logits[node_mask], field_y[node_mask])\n",
    "        boundary_loss = boundary_loss_fn(boundary_logits[node_mask], boundary_y[node_mask].float())\n",
    "        loss = field_loss + boundary_weight * boundary_loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # ---- predictions ----\n",
    "        field_pred = torch.argmax(field_logits, dim=-1)\n",
    "\n",
    "        bound_prob = torch.sigmoid(boundary_logits)       # [B, N]\n",
    "        bound_pred = (bound_prob >= threshold).long()     # [B, N]\n",
    "\n",
    "        # gather valid nodes for FIELD metrics\n",
    "        valid_field = node_mask & (field_y != -100)\n",
    "        all_field_true.extend(field_y[valid_field].detach().cpu().tolist())\n",
    "        all_field_pred.extend(field_pred[valid_field].detach().cpu().tolist())\n",
    "\n",
    "        # gather valid nodes for BOUNDARY metrics\n",
    "        all_bound_true.extend(boundary_y[node_mask].detach().cpu().long().tolist())\n",
    "        all_bound_pred.extend(bound_pred[node_mask].detach().cpu().tolist())\n",
    "\n",
    "    avg_loss = total_loss / max(1, len(loader))\n",
    "\n",
    "    print(\"=== Field Label Metrics (node-level) ===\")\n",
    "    print(classification_report(\n",
    "        all_field_true,\n",
    "        all_field_pred,\n",
    "        labels=list(range(len(LABELS))),\n",
    "        target_names=[id2label[i] for i in range(len(LABELS))],\n",
    "        digits=4,\n",
    "        zero_division=0\n",
    "    ))\n",
    "\n",
    "    p, r, f1, _ = precision_recall_fscore_support(\n",
    "        all_bound_true, all_bound_pred, average=\"binary\", zero_division=0\n",
    "    )\n",
    "    print(\"=== Boundary Metrics (node-level) ===\")\n",
    "    print(f\"threshold={threshold}\")\n",
    "    print(f\"Precision: {p:.4f}  Recall: {r:.4f}  F1: {f1:.4f}\")\n",
    "    print(f\"\\nLoss: {avg_loss:.4f}\")\n",
    "\n",
    "    return avg_loss, (p, r, f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24e5a412",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def field_metrics_fast(loader, model, device, label2id, average=\"micro\"):\n",
    "    \"\"\"\n",
    "    Computes field F1 while IGNORING:\n",
    "      - padding labels (-100)\n",
    "      - the 'Other' class (since we don't care about it for event field classification)\n",
    "\n",
    "    average: \"micro\" or \"macro\"\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    yt, yp = [], []\n",
    "\n",
    "    # Resolve the integer id for \"Other\"\n",
    "    if \"Other\" not in label2id:\n",
    "        raise ValueError(\"'Other' must exist in label2id to ignore it in metrics.\")\n",
    "    OTHER_ID = label2id[\"Other\"]\n",
    "\n",
    "    for batch in loader:\n",
    "        enc = {k: v.to(device) for k, v in batch[\"enc\"].items()}\n",
    "        node_mask = batch[\"node_mask\"].to(device).bool()\n",
    "        field_y = batch[\"field_y\"].to(device)\n",
    "\n",
    "        field_logits, _ = model(\n",
    "            enc=enc,\n",
    "            node_offsets=batch[\"node_offsets\"],\n",
    "            node_mask=node_mask,\n",
    "            tag_id=batch[\"tag_id\"].to(device),\n",
    "            parent_tag_id=batch[\"parent_tag_id\"].to(device),\n",
    "            num_feats=batch[\"num_feats\"].to(device),\n",
    "            bool_feats=batch[\"bool_feats\"].to(device)\n",
    "        )\n",
    "\n",
    "        pred = torch.argmax(field_logits, dim=-1)\n",
    "\n",
    "        # valid = real nodes, not padding, and not \"Other\"\n",
    "        valid = node_mask & (field_y != -100) & (field_y != OTHER_ID)\n",
    "\n",
    "        yt.extend(field_y[valid].detach().cpu().tolist())\n",
    "        yp.extend(pred[valid].detach().cpu().tolist())\n",
    "\n",
    "    # If a fold has no non-Other labels in val, avoid crashing / nonsense\n",
    "    if len(yt) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return f1_score(yt, yp, average=average, zero_division=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4f80074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Fold 1/5 =====\n",
      "Train pages: 10 Val pages: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 100/100 [00:00<00:00, 285.70it/s, Materializing param=transformer.layer.5.sa_layer_norm.weight]   \n",
      "\u001b[1mDistilBertModel LOAD REPORT\u001b[0m from: distilbert-base-uncased\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "vocab_layer_norm.weight | UNEXPECTED |  | \n",
      "vocab_layer_norm.bias   | UNEXPECTED |  | \n",
      "vocab_transform.weight  | UNEXPECTED |  | \n",
      "vocab_projector.bias    | UNEXPECTED |  | \n",
      "vocab_transform.bias    | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "/home/tony/DTI5902/RP/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:531: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
      "  output = torch._nested_tensor_from_mask(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05 | tr_loss=5.2767 va_loss=3.2424 best_f1=0.1149 best_th=0.40\n",
      "Epoch 10 | tr_loss=4.9377 va_loss=3.1925 best_f1=0.1982 best_th=0.45\n",
      "Epoch 15 | tr_loss=4.7454 va_loss=2.6644 best_f1=0.2785 best_th=0.45\n",
      "Epoch 20 | tr_loss=3.4520 va_loss=2.3155 best_f1=0.4138 best_th=0.50\n",
      "Fold 1 | boundary: P=0.3333 R=0.5455 F1=0.4138 (th=0.50) | field: microF1=0.6716 macroF1=0.4023\n",
      "\n",
      "===== Fold 2/5 =====\n",
      "Train pages: 10 Val pages: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 100/100 [00:00<00:00, 376.82it/s, Materializing param=transformer.layer.5.sa_layer_norm.weight]   \n",
      "\u001b[1mDistilBertModel LOAD REPORT\u001b[0m from: distilbert-base-uncased\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "vocab_layer_norm.weight | UNEXPECTED |  | \n",
      "vocab_layer_norm.bias   | UNEXPECTED |  | \n",
      "vocab_transform.weight  | UNEXPECTED |  | \n",
      "vocab_projector.bias    | UNEXPECTED |  | \n",
      "vocab_transform.bias    | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05 | tr_loss=5.4571 va_loss=4.9763 best_f1=0.1284 best_th=0.45\n",
      "Epoch 10 | tr_loss=5.4087 va_loss=4.8477 best_f1=0.1558 best_th=0.50\n",
      "Epoch 15 | tr_loss=4.3502 va_loss=4.9625 best_f1=0.1728 best_th=0.50\n",
      "Epoch 20 | tr_loss=3.3353 va_loss=5.7973 best_f1=0.1728 best_th=0.50\n",
      "Fold 2 | boundary: P=0.1000 R=0.6364 F1=0.1728 (th=0.50) | field: microF1=0.2778 macroF1=0.1380\n",
      "\n",
      "===== Fold 3/5 =====\n",
      "Train pages: 10 Val pages: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Loading weights: 100%|██████████| 100/100 [00:00<00:00, 357.88it/s, Materializing param=transformer.layer.5.sa_layer_norm.weight]   \n",
      "\u001b[1mDistilBertModel LOAD REPORT\u001b[0m from: distilbert-base-uncased\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "vocab_layer_norm.weight | UNEXPECTED |  | \n",
      "vocab_layer_norm.bias   | UNEXPECTED |  | \n",
      "vocab_transform.weight  | UNEXPECTED |  | \n",
      "vocab_projector.bias    | UNEXPECTED |  | \n",
      "vocab_transform.bias    | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05 | tr_loss=5.5637 va_loss=5.5621 best_f1=0.0983 best_th=0.45\n",
      "Epoch 10 | tr_loss=4.5645 va_loss=6.0047 best_f1=0.1020 best_th=0.40\n",
      "Epoch 15 | tr_loss=3.9580 va_loss=6.4905 best_f1=0.1493 best_th=0.50\n",
      "Epoch 20 | tr_loss=3.2677 va_loss=7.0296 best_f1=0.2368 best_th=0.55\n",
      "Fold 3 | boundary: P=0.1607 R=0.4500 F1=0.2368 (th=0.55) | field: microF1=0.4769 macroF1=0.2638\n",
      "\n",
      "===== Fold 4/5 =====\n",
      "Train pages: 11 Val pages: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 100/100 [00:00<00:00, 386.99it/s, Materializing param=transformer.layer.5.sa_layer_norm.weight]   \n",
      "\u001b[1mDistilBertModel LOAD REPORT\u001b[0m from: distilbert-base-uncased\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "vocab_layer_norm.weight | UNEXPECTED |  | \n",
      "vocab_layer_norm.bias   | UNEXPECTED |  | \n",
      "vocab_transform.weight  | UNEXPECTED |  | \n",
      "vocab_projector.bias    | UNEXPECTED |  | \n",
      "vocab_transform.bias    | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05 | tr_loss=5.4913 va_loss=11.9599 best_f1=0.4264 best_th=0.55\n",
      "Epoch 10 | tr_loss=4.8642 va_loss=11.5304 best_f1=0.4690 best_th=0.45\n",
      "Epoch 15 | tr_loss=4.2137 va_loss=10.9689 best_f1=0.5385 best_th=0.45\n",
      "Epoch 20 | tr_loss=3.0975 va_loss=11.6543 best_f1=0.5758 best_th=0.40\n",
      "Fold 4 | boundary: P=0.4578 R=0.7755 F1=0.5758 (th=0.40) | field: microF1=0.3333 macroF1=0.4269\n",
      "\n",
      "===== Fold 5/5 =====\n",
      "Train pages: 11 Val pages: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 100/100 [00:00<00:00, 471.04it/s, Materializing param=transformer.layer.5.sa_layer_norm.weight]   \n",
      "\u001b[1mDistilBertModel LOAD REPORT\u001b[0m from: distilbert-base-uncased\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "vocab_layer_norm.weight | UNEXPECTED |  | \n",
      "vocab_layer_norm.bias   | UNEXPECTED |  | \n",
      "vocab_transform.weight  | UNEXPECTED |  | \n",
      "vocab_projector.bias    | UNEXPECTED |  | \n",
      "vocab_transform.bias    | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05 | tr_loss=5.0488 va_loss=8.3533 best_f1=0.2276 best_th=0.35\n",
      "Epoch 10 | tr_loss=4.7132 va_loss=8.5873 best_f1=0.2500 best_th=0.35\n",
      "Epoch 15 | tr_loss=4.0132 va_loss=9.1726 best_f1=0.2616 best_th=0.30\n",
      "Epoch 20 | tr_loss=2.7902 va_loss=10.3072 best_f1=0.2857 best_th=0.25\n",
      "Fold 5 | boundary: P=0.1742 R=0.7941 F1=0.2857 (th=0.25) | field: microF1=0.1813 macroF1=0.1016\n",
      "\n",
      "===== CV Summary =====\n",
      "Boundary  Mean F1: 0.3370\n",
      "Boundary  Mean P : 0.2452\n",
      "Boundary  Mean R : 0.6403\n",
      "Boundary  Mean th: 0.4400\n",
      "Field     Mean micro-F1: 0.3882\n",
      "Field     Mean macro-F1: 0.2665\n",
      "Using CV-avg threshold: 0.43999999999999995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 100/100 [00:00<00:00, 327.31it/s, Materializing param=transformer.layer.5.sa_layer_norm.weight]   \n",
      "\u001b[1mDistilBertModel LOAD REPORT\u001b[0m from: distilbert-base-uncased\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "vocab_layer_norm.weight | UNEXPECTED |  | \n",
      "vocab_layer_norm.bias   | UNEXPECTED |  | \n",
      "vocab_transform.weight  | UNEXPECTED |  | \n",
      "vocab_projector.bias    | UNEXPECTED |  | \n",
      "vocab_transform.bias    | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FINAL] Epoch 05 | tr_loss=5.1852\n",
      "[FINAL] Epoch 10 | tr_loss=4.3720\n",
      "[FINAL] Epoch 15 | tr_loss=3.7728\n",
      "[FINAL] Epoch 20 | tr_loss=2.7044\n",
      "\n",
      "===== HOLDOUT TEST (Boundary) =====\n",
      "Threshold=0.44  P=0.0796  R=0.1731  F1=0.1091\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 2) Helper functions to build loaders, model, losses per fold\n",
    "# -------------------------------\n",
    "def make_loaders(train_df, val_df, batch_size=2, max_tokens=64):\n",
    "    train_dataset = PageDataset(train_df, tokenizer=tokenizer, max_tokens=max_tokens)\n",
    "    val_dataset   = PageDataset(val_df, tokenizer=tokenizer, max_tokens=max_tokens)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "    return train_loader, val_loader\n",
    "\n",
    "def init_model_and_optim(lr=1e-5):\n",
    "    model = DOMAwareEventExtractor(\n",
    "        text_model_name=MODEL_NAME,\n",
    "        num_field_labels=len(LABELS),\n",
    "        tag_vocab_size=len(TAG_VOCAB),\n",
    "        parent_tag_vocab_size=len(PARENT_TAG_VOCAB),\n",
    "        d_model=128,\n",
    "        nhead=4,\n",
    "        num_layers=2\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "    return model, optimizer\n",
    "\n",
    "def make_losses_for_fold(train_df):\n",
    "    # ---- FIELD weights computed on TRAIN ONLY ----\n",
    "    from collections import Counter\n",
    "    label_counts = Counter(train_df[\"label\"])\n",
    "    total = sum(label_counts.values())\n",
    "    weights = []\n",
    "    for label in LABELS:\n",
    "        c = label_counts.get(label, 1)\n",
    "        weights.append(total / c)\n",
    "    weights = torch.tensor(weights, dtype=torch.float32).to(device)\n",
    "\n",
    "    field_loss_fn = torch.nn.CrossEntropyLoss(weight=weights, ignore_index=-100)\n",
    "\n",
    "    # ---- BOUNDARY pos_weight computed on TRAIN ONLY ----\n",
    "    pos = float(train_df[\"start_event\"].sum())\n",
    "    neg = float(len(train_df) - pos)\n",
    "    pos_weight = torch.tensor([neg / (pos + 1e-6)], dtype=torch.float32).to(device)\n",
    "    boundary_loss_fn = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "    return field_loss_fn, boundary_loss_fn\n",
    "\n",
    "# -------------------------------\n",
    "# 3) Training + evaluation per fold\n",
    "# -------------------------------\n",
    "def run_epoch(model, optimizer, loader, field_loss_fn, boundary_loss_fn, boundary_weight=3.0, training=True):\n",
    "    model.train() if training else model.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in loader:\n",
    "        enc = {k: v.to(device) for k, v in batch[\"enc\"].items()}\n",
    "        node_mask = batch[\"node_mask\"].to(device).bool()\n",
    "\n",
    "        tag_id = batch[\"tag_id\"].to(device)\n",
    "        parent_tag_id = batch[\"parent_tag_id\"].to(device)\n",
    "        num_feats = batch[\"num_feats\"].to(device)\n",
    "        bool_feats = batch[\"bool_feats\"].to(device)\n",
    "\n",
    "        field_y = batch[\"field_y\"].to(device)\n",
    "        boundary_y = batch[\"boundary_y\"].to(device)\n",
    "\n",
    "        with torch.set_grad_enabled(training):\n",
    "            field_logits, boundary_logits = model(\n",
    "                enc=enc,\n",
    "                node_offsets=batch[\"node_offsets\"],\n",
    "                node_mask=node_mask,\n",
    "                tag_id=tag_id,\n",
    "                parent_tag_id=parent_tag_id,\n",
    "                num_feats=num_feats,\n",
    "                bool_feats=bool_feats\n",
    "            )\n",
    "\n",
    "            field_mask = node_mask & (field_y != -100) & (field_y != OTHER_ID)\n",
    "            field_loss = field_loss_fn(field_logits[field_mask], field_y[field_mask])\n",
    "            boundary_loss = boundary_loss_fn(boundary_logits[node_mask], boundary_y[node_mask].float())\n",
    "            loss = field_loss + boundary_weight * boundary_loss\n",
    "\n",
    "            if training:\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "        total_loss += loss.detach().item()\n",
    "\n",
    "    return total_loss / max(1, len(loader))\n",
    "\n",
    "@torch.no_grad()\n",
    "def find_best_threshold(loader, model, suppress_k=0):\n",
    "    # suppression should be OFF now; keep param for compatibility\n",
    "    model.eval()\n",
    "\n",
    "    page_probs, page_true = [], []\n",
    "\n",
    "    for batch in loader:\n",
    "        enc = {k:v.to(device) for k,v in batch[\"enc\"].items()}\n",
    "        node_mask = batch[\"node_mask\"].to(device).bool()\n",
    "\n",
    "        _, boundary_logits = model(\n",
    "            enc=enc,\n",
    "            node_offsets=batch[\"node_offsets\"],\n",
    "            node_mask=node_mask,\n",
    "            tag_id=batch[\"tag_id\"].to(device),\n",
    "            parent_tag_id=batch[\"parent_tag_id\"].to(device),\n",
    "            num_feats=batch[\"num_feats\"].to(device),\n",
    "            bool_feats=batch[\"bool_feats\"].to(device)\n",
    "        )\n",
    "\n",
    "        probs = torch.sigmoid(boundary_logits).detach().cpu()\n",
    "        true  = batch[\"boundary_y\"].long().cpu()\n",
    "        mask  = node_mask.detach().cpu()\n",
    "\n",
    "        B, N = probs.shape\n",
    "        for b in range(B):\n",
    "            valid_idx = torch.where(mask[b])[0]\n",
    "            if valid_idx.numel() == 0:\n",
    "                continue\n",
    "            page_probs.append(probs[b, valid_idx].numpy())\n",
    "            page_true.append(true[b, valid_idx].numpy().astype(int))\n",
    "\n",
    "    best_th, best_f1 = 0.5, -1.0\n",
    "\n",
    "    for th in np.linspace(0.05, 0.95, 19):\n",
    "        pred_flat, true_flat = [], []\n",
    "        for p, t in zip(page_probs, page_true):\n",
    "            pred = (p >= th).astype(int)\n",
    "            pred_flat.append(pred)\n",
    "            true_flat.append(t)\n",
    "\n",
    "        if len(pred_flat) == 0:\n",
    "            continue\n",
    "\n",
    "        pred_flat = np.concatenate(pred_flat)\n",
    "        true_flat = np.concatenate(true_flat)\n",
    "\n",
    "        tp = ((pred_flat == 1) & (true_flat == 1)).sum()\n",
    "        fp = ((pred_flat == 1) & (true_flat == 0)).sum()\n",
    "        fn = ((pred_flat == 0) & (true_flat == 1)).sum()\n",
    "\n",
    "        precision = tp / (tp + fp + 1e-9)\n",
    "        recall    = tp / (tp + fn + 1e-9)\n",
    "        f1        = 2 * precision * recall / (precision + recall + 1e-9)\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_th = f1, th\n",
    "\n",
    "    return best_th, best_f1\n",
    "\n",
    "@torch.no_grad()\n",
    "def boundary_metrics(loader, model, threshold):\n",
    "    model.eval()\n",
    "    all_true, all_pred = [], []\n",
    "\n",
    "    for batch in loader:\n",
    "        enc = {k:v.to(device) for k,v in batch[\"enc\"].items()}\n",
    "        node_mask = batch[\"node_mask\"].to(device).bool()\n",
    "\n",
    "        _, boundary_logits = model(\n",
    "            enc=enc,\n",
    "            node_offsets=batch[\"node_offsets\"],\n",
    "            node_mask=node_mask,\n",
    "            tag_id=batch[\"tag_id\"].to(device),\n",
    "            parent_tag_id=batch[\"parent_tag_id\"].to(device),\n",
    "            num_feats=batch[\"num_feats\"].to(device),\n",
    "            bool_feats=batch[\"bool_feats\"].to(device)\n",
    "        )\n",
    "\n",
    "        prob = torch.sigmoid(boundary_logits)\n",
    "        pred = (prob >= threshold).long()\n",
    "\n",
    "        y = batch[\"boundary_y\"].to(device).long()\n",
    "\n",
    "        all_true.extend(y[node_mask].detach().cpu().tolist())\n",
    "        all_pred.extend(pred[node_mask].detach().cpu().tolist())\n",
    "\n",
    "    from sklearn.metrics import precision_recall_fscore_support\n",
    "    p, r, f1, _ = precision_recall_fscore_support(all_true, all_pred, average=\"binary\", zero_division=0)\n",
    "    return p, r, f1\n",
    "\n",
    "# -------------------------------\n",
    "# 4) Run K-Fold CV by source\n",
    "# -------------------------------\n",
    "N_SPLITS = min(5, len(cv_sources))  # safety\n",
    "kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "\n",
    "cv_results = []\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(kf.split(cv_sources), start=1):\n",
    "    fold_train_sources = set(cv_sources[tr_idx])\n",
    "    fold_val_sources   = set(cv_sources[va_idx])\n",
    "\n",
    "    fold_train_df = df[df[\"source\"].isin(fold_train_sources)].copy()\n",
    "    fold_val_df   = df[df[\"source\"].isin(fold_val_sources)].copy()\n",
    "\n",
    "    field_loss_fn, boundary_loss_fn = make_losses_for_train_df(fold_train_df, LABELS, device, other_scale=0.01, weight_cap=50.0)\n",
    "\n",
    "    print(f\"\\n===== Fold {fold}/{N_SPLITS} =====\")\n",
    "    print(\"Train pages:\", fold_train_df[\"source\"].nunique(), \"Val pages:\", fold_val_df[\"source\"].nunique())\n",
    "\n",
    "    train_loader, val_loader = make_loaders(fold_train_df, fold_val_df, batch_size=2)\n",
    "\n",
    "    model, optimizer = init_model_and_optim(lr=1e-5)\n",
    "\n",
    "    best = {\"f1\": -1.0, \"th\": 0.5, \"state\": None}\n",
    "\n",
    "    EPOCHS = 20\n",
    "    for epoch in range(EPOCHS):\n",
    "        tr_loss = run_epoch(model, optimizer, train_loader, field_loss_fn, boundary_loss_fn,\n",
    "                            boundary_weight=3.0, training=True)\n",
    "        va_loss = run_epoch(model, optimizer, val_loader, field_loss_fn, boundary_loss_fn,\n",
    "                            boundary_weight=3.0, training=False)\n",
    "\n",
    "        th, f1 = find_best_threshold(val_loader, model, suppress_k=0)\n",
    "\n",
    "        if f1 > best[\"f1\"]:\n",
    "            best[\"f1\"] = f1\n",
    "            best[\"th\"] = th\n",
    "            best[\"state\"] = copy.deepcopy(model.state_dict())\n",
    " \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1:02d} | tr_loss={tr_loss:.4f} va_loss={va_loss:.4f} best_f1={best['f1']:.4f} best_th={best['th']:.2f}\")\n",
    "\n",
    "    # load best and report metrics on fold val\n",
    "    model.load_state_dict(best[\"state\"])\n",
    "    bp, br, bf1 = boundary_metrics(val_loader, model, threshold=best[\"th\"])\n",
    "\n",
    "    # Field (fast summary only)\n",
    "    field_micro_f1 = field_metrics_fast(val_loader, model, device, label2id=label2id, average=\"micro\")\n",
    "    field_macro_f1 = field_metrics_fast(val_loader, model, device, label2id=label2id, average=\"macro\")\n",
    "\n",
    "    print(\n",
    "        f\"Fold {fold} | boundary: P={bp:.4f} R={br:.4f} F1={bf1:.4f} (th={best['th']:.2f})\"\n",
    "        f\" | field: microF1={field_micro_f1:.4f} macroF1={field_macro_f1:.4f}\"\n",
    "    )\n",
    "\n",
    "    cv_results.append({\n",
    "        \"fold\": fold,\n",
    "        \"bp\": bp, \"br\": br, \"bf1\": bf1, \"th\": best[\"th\"],\n",
    "        \"field_micro_f1\": field_micro_f1,\n",
    "        \"field_macro_f1\": field_macro_f1\n",
    "    })\n",
    "\n",
    "print(\"\\n===== CV Summary =====\")\n",
    "\n",
    "mean_bf1 = float(np.mean([x[\"bf1\"] for x in cv_results]))\n",
    "mean_bp  = float(np.mean([x[\"bp\"]  for x in cv_results]))\n",
    "mean_br  = float(np.mean([x[\"br\"]  for x in cv_results]))\n",
    "mean_th  = float(np.mean([x[\"th\"]  for x in cv_results]))\n",
    "\n",
    "mean_field_micro = float(np.mean([x[\"field_micro_f1\"] for x in cv_results]))\n",
    "mean_field_macro = float(np.mean([x[\"field_macro_f1\"] for x in cv_results]))\n",
    "\n",
    "print(f\"Boundary  Mean F1: {mean_bf1:.4f}\")\n",
    "print(f\"Boundary  Mean P : {mean_bp:.4f}\")\n",
    "print(f\"Boundary  Mean R : {mean_br:.4f}\")\n",
    "print(f\"Boundary  Mean th: {mean_th:.4f}\")\n",
    "\n",
    "print(f\"Field     Mean micro-F1: {mean_field_micro:.4f}\")\n",
    "print(f\"Field     Mean macro-F1: {mean_field_macro:.4f}\")\n",
    "\n",
    "best_th_cv = mean_th\n",
    "print(\"Using CV-avg threshold:\", best_th_cv)\n",
    "\n",
    "# -------------------------------\n",
    "# 5) Final training on ALL CV pages, then evaluate on HOLDOUT TEST pages\n",
    "# -------------------------------\n",
    "final_train_df = df[df[\"source\"].isin(set(cv_sources))].copy()\n",
    "\n",
    "final_train_loader, _ = make_loaders(final_train_df, final_train_df, batch_size=2)  # dummy val loader\n",
    "test_dataset = PageDataset(test_df, tokenizer)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "field_loss_fn, boundary_loss_fn = make_losses_for_train_df(\n",
    "    final_train_df, LABELS, device, other_scale=0.01, weight_cap=50.0\n",
    ")\n",
    "final_model, final_optimizer = init_model_and_optim(lr=1e-5)\n",
    "\n",
    "EPOCHS_FINAL = 20\n",
    "for epoch in range(EPOCHS_FINAL):\n",
    "    tr_loss = run_epoch(final_model, final_optimizer, final_train_loader,\n",
    "                        field_loss_fn, boundary_loss_fn, boundary_weight=3.0, training=True)\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"[FINAL] Epoch {epoch+1:02d} | tr_loss={tr_loss:.4f}\")\n",
    "\n",
    "p, r, f1 = boundary_metrics(test_loader, final_model, threshold=best_th_cv)\n",
    "print(\"\\n===== HOLDOUT TEST (Boundary) =====\")\n",
    "print(f\"Threshold={best_th_cv:.2f}  P={p:.4f}  R={r:.4f}  F1={f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc149ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holdout test sources: [np.str_('members.sacac.org_pattern_labeled'), np.str_('nacacnet.org_pattern_labeled')]\n",
      "Holdout start_event positives (total): 52\n",
      "Holdout start_event positives per page:\n",
      " source\n",
      "members.sacac.org_pattern_labeled    30\n",
      "nacacnet.org_pattern_labeled         22\n",
      "Name: start_event, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Holdout test sources:\", sorted(list(test_sources)))\n",
    "\n",
    "print(\"Holdout start_event positives (total):\", int(test_df[\"start_event\"].sum()))\n",
    "\n",
    "per_page = test_df.groupby(\"source\")[\"start_event\"].sum().sort_values(ascending=False)\n",
    "print(\"Holdout start_event positives per page:\\n\", per_page.astype(int))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20298936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best holdout threshold: 0.1 Best holdout F1: 0.1496402876309094\n"
     ]
    }
   ],
   "source": [
    "best_th_test, best_f1_test = find_best_threshold(test_loader, final_model, suppress_k=0)\n",
    "print(\"Best holdout threshold:\", best_th_test, \"Best holdout F1:\", best_f1_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
